{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dpfa_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV0I2bmm3iRy"
      },
      "source": [
        "This scripts reprodcue the results in the paper:\n",
        "```\n",
        "@inproceedings{pu2021deep,\n",
        "  title={Deep Performance Factors Analysis for Knowledge Tracing},\n",
        "  author={Pu, Shi and Converse, Geoffrey and Huang, Yuchi},\n",
        "  booktitle={International Conference on Artificial Intelligence in Education},\n",
        "  pages={331--341},\n",
        "  year={2021},\n",
        "  organization={Springer}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhjLITmWVEfs"
      },
      "source": [
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "\n",
        "BASE_DIR = '/content/dirve'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehY69GZzVdvK",
        "outputId": "3699bf7b-a3b1-4b01-b06f-eea4dcc9bcf1"
      },
      "source": [
        "# check if GPU is visible\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tldVb19VhZh",
        "outputId": "19374ded-1caf-4769-c96f-4e8be6289526"
      },
      "source": [
        "# mount google drive to access data\n",
        "drive.mount(BASE_DIR, force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/dirve\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zIRI9-QVido"
      },
      "source": [
        "# change to the destination dir\n",
        "os.chdir(os.path.join(BASE_DIR, 'MyDrive', 'dpfa', 'dpfa'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AMO-tcMaXbqd",
        "outputId": "74fa14c8-3fc3-4485-e9b0-4d5b15bcd950"
      },
      "source": [
        "# only run once at start\n",
        "!pip install -r ../requirements.txt "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py==0.12.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: attrs==21.2.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 3)) (21.2.0)\n",
            "Requirement already satisfied: cached-property==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 5)) (4.2.2)\n",
            "Requirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 6)) (2021.5.30)\n",
            "Collecting charset-normalizer==2.0.3\n",
            "  Downloading charset_normalizer-2.0.3-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 8)) (0.10.0)\n",
            "Collecting Cython==0.29.24\n",
            "  Downloading Cython-0.29.24-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 21.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill==0.3.4 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 10)) (0.3.4)\n",
            "Requirement already satisfied: dm-tree==0.1.6 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 11)) (0.1.6)\n",
            "Requirement already satisfied: flatbuffers==1.12 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 12)) (1.12)\n",
            "Collecting future==0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 70.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 14)) (0.4.0)\n",
            "Requirement already satisfied: gin-config==0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 15)) (0.4.0)\n",
            "Collecting google-api-core==1.31.0\n",
            "  Downloading google_api_core-1.31.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting google-api-python-client==2.14.0\n",
            "  Downloading google_api_python_client-2.14.0-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 43.4 MB/s \n",
            "\u001b[?25hCollecting google-auth==1.33.1\n",
            "  Downloading google_auth-1.33.1-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[K     |████████████████████████████████| 152 kB 77.7 MB/s \n",
            "\u001b[?25hCollecting google-auth-httplib2==0.1.0\n",
            "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: google-auth-oauthlib==0.4.4 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 20)) (0.4.4)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 21)) (0.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos==1.53.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 22)) (1.53.0)\n",
            "Requirement already satisfied: grpcio==1.34.1 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 23)) (1.34.1)\n",
            "Requirement already satisfied: h5py==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 24)) (3.1.0)\n",
            "Collecting httplib2==0.19.1\n",
            "  Downloading httplib2-0.19.1-py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting idna==3.2\n",
            "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata==4.6.1 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 27)) (4.6.1)\n",
            "Requirement already satisfied: importlib-resources==5.2.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 28)) (5.2.0)\n",
            "Requirement already satisfied: joblib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 29)) (1.0.1)\n",
            "Requirement already satisfied: kaggle==1.5.12 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 30)) (1.5.12)\n",
            "Requirement already satisfied: keras-nightly==2.5.0.dev2021032900 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 31)) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 32)) (1.1.2)\n",
            "Requirement already satisfied: kiwisolver==1.3.1 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 33)) (1.3.1)\n",
            "Requirement already satisfied: Markdown==3.3.4 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 34)) (3.3.4)\n",
            "Collecting matplotlib==3.4.2\n",
            "  Downloading matplotlib-3.4.2-cp37-cp37m-manylinux1_x86_64.whl (10.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3 MB 72.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 36)) (1.19.5)\n",
            "Requirement already satisfied: oauth2client==4.1.3 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 37)) (4.1.3)\n",
            "Requirement already satisfied: oauthlib==3.1.1 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 38)) (3.1.1)\n",
            "Collecting opencv-python-headless==4.5.3.56\n",
            "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 83 kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 40)) (3.3.0)\n",
            "Requirement already satisfied: packaging==21.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 41)) (21.0)\n",
            "Collecting pandas==1.3.0\n",
            "  Downloading pandas-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (10.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.8 MB 73.8 MB/s \n",
            "\u001b[?25hCollecting Pillow==8.3.1\n",
            "  Downloading Pillow-8.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 60.0 MB/s \n",
            "\u001b[?25hCollecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: promise==2.3 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 45)) (2.3)\n",
            "Requirement already satisfied: protobuf==3.17.3 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 46)) (3.17.3)\n",
            "Collecting psutil==5.8.0\n",
            "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 65.0 MB/s \n",
            "\u001b[?25hCollecting py-cpuinfo==8.0.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 12.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 49)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 50)) (0.2.8)\n",
            "Requirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 51)) (2.0.2)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 52)) (2.4.7)\n",
            "Collecting python-dateutil==2.8.2\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 70.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-slugify==5.0.2 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 54)) (5.0.2)\n",
            "Collecting pytz==2021.1\n",
            "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 80.6 MB/s \n",
            "\u001b[?25hCollecting PyYAML==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 63.4 MB/s \n",
            "\u001b[?25hCollecting requests==2.26.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 941 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 58)) (1.3.0)\n",
            "Requirement already satisfied: rsa==4.7.2 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 59)) (4.7.2)\n",
            "Collecting sacrebleu==1.5.1\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting scipy==1.7.0\n",
            "  Downloading scipy-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 29 kB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.96\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 70.0 MB/s \n",
            "\u001b[?25hCollecting seqeval==1.2.2\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 65)) (1.15.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 66)) (0.0)\n",
            "Requirement already satisfied: tensorboard==2.5.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 67)) (2.5.0)\n",
            "Requirement already satisfied: tensorboard-data-server==0.6.1 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 68)) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit==1.8.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 69)) (1.8.0)\n",
            "Requirement already satisfied: tensorflow==2.5.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 70)) (2.5.0)\n",
            "Collecting tensorflow-addons==0.13.0\n",
            "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 56.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow-datasets==4.3.0\n",
            "  Downloading tensorflow_datasets-4.3.0-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 56.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator==2.5.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 73)) (2.5.0)\n",
            "Requirement already satisfied: tensorflow-hub==0.12.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 74)) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-metadata==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 75)) (1.1.0)\n",
            "Collecting tensorflow-model-optimization==0.6.0\n",
            "  Downloading tensorflow_model_optimization-0.6.0-py2.py3-none-any.whl (211 kB)\n",
            "\u001b[K     |████████████████████████████████| 211 kB 54.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 77)) (1.1.0)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 78)) (1.3)\n",
            "Collecting tf-models-official==2.5.1\n",
            "  Downloading tf_models_official-2.5.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 50.1 MB/s \n",
            "\u001b[?25hCollecting tf-slim==1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 64.3 MB/s \n",
            "\u001b[?25hCollecting threadpoolctl==2.2.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Collecting tqdm==4.61.2\n",
            "  Downloading tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting typeguard==2.12.1\n",
            "  Downloading typeguard-2.12.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: typing-extensions==3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 84)) (3.7.4.3)\n",
            "Requirement already satisfied: uritemplate==3.0.1 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 85)) (3.0.1)\n",
            "Collecting urllib3==1.26.6\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 80.7 MB/s \n",
            "\u001b[?25hCollecting Werkzeug==2.0.1\n",
            "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 62.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 88)) (1.12.1)\n",
            "Requirement already satisfied: zipp==3.5.0 in /usr/local/lib/python3.7/dist-packages (from -r ../requirements.txt (line 89)) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse==1.6.3->-r ../requirements.txt (line 2)) (0.36.2)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core==1.31.0->-r ../requirements.txt (line 16)) (57.2.0)\n",
            "Building wheels for collected packages: future, py-cpuinfo, seqeval\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=67a7b2beca257254d62213929808c4c2c4cde2b0abc187bbaf8c45e10abfd7e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=a97cfc692589cdde6e46d905b37d17ef37e518edfd74a71f9a17ff104099fcb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=394539f5acbf8387204f445a08efc2db94784b4cedace7b6251f7016d6438771\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built future py-cpuinfo seqeval\n",
            "Installing collected packages: urllib3, idna, charset-normalizer, requests, google-auth, Werkzeug, threadpoolctl, scipy, pytz, python-dateutil, Pillow, httplib2, typeguard, tqdm, scikit-learn, portalocker, matplotlib, google-auth-httplib2, google-api-core, future, Cython, tf-slim, tensorflow-model-optimization, tensorflow-datasets, tensorflow-addons, seqeval, sentencepiece, sacrebleu, PyYAML, py-cpuinfo, psutil, pandas, opencv-python-headless, google-api-python-client, tf-models-official\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.0.2\n",
            "    Uninstalling charset-normalizer-2.0.2:\n",
            "      Successfully uninstalled charset-normalizer-2.0.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 1.32.1\n",
            "    Uninstalling google-auth-1.32.1:\n",
            "      Successfully uninstalled google-auth-1.32.1\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.1\n",
            "    Uninstalling python-dateutil-2.8.1:\n",
            "      Successfully uninstalled python-dateutil-2.8.1\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: httplib2\n",
            "    Found existing installation: httplib2 0.17.4\n",
            "    Uninstalling httplib2-0.17.4:\n",
            "      Successfully uninstalled httplib2-0.17.4\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 2.7.1\n",
            "    Uninstalling typeguard-2.7.1:\n",
            "      Successfully uninstalled typeguard-2.7.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: google-auth-httplib2\n",
            "    Found existing installation: google-auth-httplib2 0.0.4\n",
            "    Uninstalling google-auth-httplib2-0.0.4:\n",
            "      Successfully uninstalled google-auth-httplib2-0.0.4\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.26.3\n",
            "    Uninstalling google-api-core-1.26.3:\n",
            "      Successfully uninstalled google-api-core-1.26.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 0.29.23\n",
            "    Uninstalling Cython-0.29.23:\n",
            "      Successfully uninstalled Cython-0.29.23\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.0.1\n",
            "    Uninstalling tensorflow-datasets-4.0.1:\n",
            "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.0.1 which is incompatible.\n",
            "earthengine-api 0.1.272 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 2.14.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Cython-0.29.24 Pillow-8.3.1 PyYAML-5.4.1 Werkzeug-2.0.1 charset-normalizer-2.0.3 future-0.18.2 google-api-core-1.31.0 google-api-python-client-2.14.0 google-auth-1.33.1 google-auth-httplib2-0.1.0 httplib2-0.19.1 idna-3.2 matplotlib-3.4.2 opencv-python-headless-4.5.3.56 pandas-1.3.0 portalocker-2.0.0 psutil-5.8.0 py-cpuinfo-8.0.0 python-dateutil-2.8.2 pytz-2021.1 requests-2.26.0 sacrebleu-1.5.1 scikit-learn-0.24.2 scipy-1.7.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorflow-addons-0.13.0 tensorflow-datasets-4.3.0 tensorflow-model-optimization-0.6.0 tf-models-official-2.5.1 tf-slim-1.1.0 threadpoolctl-2.2.0 tqdm-4.61.2 typeguard-2.12.1 urllib3-1.26.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "dateutil",
                  "google",
                  "google_auth_httplib2",
                  "googleapiclient",
                  "httplib2",
                  "idna",
                  "matplotlib",
                  "mpl_toolkits",
                  "pandas",
                  "psutil",
                  "pytz",
                  "requests",
                  "scipy",
                  "urllib3",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg3x3usdtz0i",
        "outputId": "3ce1eb67-70a2-4900-cfda-5fd40e22f6d2"
      },
      "source": [
        "# to use default (optimal configuration), only adjust the dataset and single_attempts_only configuration\n",
        "!python main.py --dataset 'synthetic5'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-27 00:31:38.766001: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "fit params are {'batch_size': 500, 'epochs': 100, 'validation_split': 0.1}\n",
            "optimizer and loss params = {'lr': 0.16, 'smoothing': 0.1}\n",
            "session cleared\n",
            "\n",
            " cv 0 starts for synthetic5\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/synthetic5-cv-train-0.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/synthetic5-cv-test-0.csv is loaded\n",
            "dataset = synthetic5\n",
            "num of attempts = 200.0 K\n",
            "num of students = 4000\n",
            "num of items = 52\n",
            "input dataset is already a feature datasetskipping the encoding and feature extraction step\n",
            "100% 3200/3200 [00:00<00:00, 116199.79it/s]\n",
            "100% 800/800 [00:00<00:00, 144543.95it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "2021-07-27 00:31:41.934795: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-27 00:31:41.942799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:31:41.943419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-07-27 00:31:41.943458: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-27 00:31:41.946061: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-07-27 00:31:41.946166: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-07-27 00:31:41.947724: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-27 00:31:41.948078: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-27 00:31:41.949541: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-27 00:31:41.950153: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-07-27 00:31:41.950345: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-07-27 00:31:41.950465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:31:41.951202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:31:41.951794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-07-27 00:31:41.952138: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-07-27 00:31:41.952472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:31:41.953111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-07-27 00:31:41.953193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:31:41.953788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:31:41.954341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-07-27 00:31:41.954423: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-27 00:31:42.468094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-27 00:31:42.468153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-07-27 00:31:42.468164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-07-27 00:31:42.468405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:31:42.469271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:31:42.469960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:31:42.470531: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-07-27 00:31:42.470575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14682 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 52, 'dropout': 0.2, 'regulate_dot_product': False, 'normalize_embedding': False, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "2021-07-27 00:31:42.529354: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-07-27 00:31:42.529795: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000165000 Hz\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "2021-07-27 00:31:43.809152: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-07-27 00:31:44.205587: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "6/6 [==============================] - 2s 43ms/step - loss: 0.6885 - val_loss: 0.6213\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.6160 - val_loss: 0.5947\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5994 - val_loss: 0.5831\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5904 - val_loss: 0.5766\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5830 - val_loss: 0.5668\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5743 - val_loss: 0.5614\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5672 - val_loss: 0.5561\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5621 - val_loss: 0.5527\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5584 - val_loss: 0.5492\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5551 - val_loss: 0.5475\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5535 - val_loss: 0.5459\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5514 - val_loss: 0.5437\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5494 - val_loss: 0.5430\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5485 - val_loss: 0.5408\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5469 - val_loss: 0.5408\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5466 - val_loss: 0.5412\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5460 - val_loss: 0.5395\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5451 - val_loss: 0.5382\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5443 - val_loss: 0.5374\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5435 - val_loss: 0.5373\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5427 - val_loss: 0.5355\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5423 - val_loss: 0.5334\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5404 - val_loss: 0.5334\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5399 - val_loss: 0.5315\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5401 - val_loss: 0.5310\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5392 - val_loss: 0.5324\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5388 - val_loss: 0.5312\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5309\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5389 - val_loss: 0.5316\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5390 - val_loss: 0.5312\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5390 - val_loss: 0.5317\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5316\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5389 - val_loss: 0.5304\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5305\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5387 - val_loss: 0.5311\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5303\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5378 - val_loss: 0.5304\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5307\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5301\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5318\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5299\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5307\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5302\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5306\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5309\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5299\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5304\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5377 - val_loss: 0.5310\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5388 - val_loss: 0.5310\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5389 - val_loss: 0.5303\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5313\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5388 - val_loss: 0.5303\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5306\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5307\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5378 - val_loss: 0.5307\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5301\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5308\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5304\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5378 - val_loss: 0.5305\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5301\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5308\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5311\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5305\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5314\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5308\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5308\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5306\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5310\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5318\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5384 - val_loss: 0.5306\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5383 - val_loss: 0.5303\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5305\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5303\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5303\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5308\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5306\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5378 - val_loss: 0.5300\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5377 - val_loss: 0.5305\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5305\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5304\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5313\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5384 - val_loss: 0.5306\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5308\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5308\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5301\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5379 - val_loss: 0.5308\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5305\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5313\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5378 - val_loss: 0.5300\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5380 - val_loss: 0.5309\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5379 - val_loss: 0.5299\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5377 - val_loss: 0.5306\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5306\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5302\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5305\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5374 - val_loss: 0.5305\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5302\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5303\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5376 - val_loss: 0.5312\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5380 - val_loss: 0.5315\n",
            "test auc = 0.8322504804434603 in cv = 0\n",
            "\n",
            " cv 0 finished for synthetic5\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 1 starts for synthetic5\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/synthetic5-cv-train-1.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/synthetic5-cv-test-1.csv is loaded\n",
            "input dataset is already a feature datasetskipping the encoding and feature extraction step\n",
            "100% 3200/3200 [00:00<00:00, 113282.07it/s]\n",
            "100% 800/800 [00:00<00:00, 129704.03it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 52, 'dropout': 0.2, 'regulate_dot_product': False, 'normalize_embedding': False, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/100\n",
            "6/6 [==============================] - 1s 36ms/step - loss: 0.7061 - val_loss: 0.6389\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.6249 - val_loss: 0.6111\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.6076 - val_loss: 0.6015\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.6000 - val_loss: 0.5896\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5932 - val_loss: 0.5859\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5896 - val_loss: 0.5829\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5849 - val_loss: 0.5779\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5811 - val_loss: 0.5752\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5781 - val_loss: 0.5734\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5757 - val_loss: 0.5707\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5732 - val_loss: 0.5703\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5705 - val_loss: 0.5628\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5686 - val_loss: 0.5653\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5663 - val_loss: 0.5594\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5628 - val_loss: 0.5535\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.5600 - val_loss: 0.5545\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5582 - val_loss: 0.5529\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5568 - val_loss: 0.5491\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5540 - val_loss: 0.5481\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5520 - val_loss: 0.5456\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5502 - val_loss: 0.5451\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5481 - val_loss: 0.5444\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5473 - val_loss: 0.5412\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5457 - val_loss: 0.5410\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5445 - val_loss: 0.5405\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5448 - val_loss: 0.5401\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5441 - val_loss: 0.5387\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5443 - val_loss: 0.5391\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5429 - val_loss: 0.5394\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5435 - val_loss: 0.5390\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5429 - val_loss: 0.5389\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5423 - val_loss: 0.5368\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5407 - val_loss: 0.5344\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5401 - val_loss: 0.5341\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5395 - val_loss: 0.5337\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5394 - val_loss: 0.5348\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5390 - val_loss: 0.5342\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5390 - val_loss: 0.5338\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5337\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5325\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5382 - val_loss: 0.5332\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5334\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5330\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5381 - val_loss: 0.5326\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5383 - val_loss: 0.5332\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5334\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5334\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5331\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5385 - val_loss: 0.5334\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5333\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5332\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5333\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5329\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5385 - val_loss: 0.5323\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5337\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5326\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5339\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5337\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5329\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5335\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5377 - val_loss: 0.5331\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5376 - val_loss: 0.5322\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5338\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5331\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5338\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5334\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5376 - val_loss: 0.5333\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5324\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5375 - val_loss: 0.5333\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5378 - val_loss: 0.5326\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5324\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5376 - val_loss: 0.5337\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5324\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5380 - val_loss: 0.5329\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5334\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5323\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5376 - val_loss: 0.5329\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5377 - val_loss: 0.5335\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5332\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5378 - val_loss: 0.5334\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5375 - val_loss: 0.5331\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5374 - val_loss: 0.5326\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5377 - val_loss: 0.5327\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.5383 - val_loss: 0.5324\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5376 - val_loss: 0.5325\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5379 - val_loss: 0.5329\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5333\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5380 - val_loss: 0.5339\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5338\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5389 - val_loss: 0.5340\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5386 - val_loss: 0.5334\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5386 - val_loss: 0.5334\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5334\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5329\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.5384 - val_loss: 0.5323\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5389 - val_loss: 0.5336\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5385 - val_loss: 0.5329\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5379 - val_loss: 0.5339\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5324\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5329\n",
            "test auc = 0.8336345325516021 in cv = 1\n",
            "\n",
            " cv 1 finished for synthetic5\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 2 starts for synthetic5\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/synthetic5-cv-train-2.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/synthetic5-cv-test-2.csv is loaded\n",
            "input dataset is already a feature datasetskipping the encoding and feature extraction step\n",
            "100% 3200/3200 [00:00<00:00, 115604.28it/s]\n",
            "100% 800/800 [00:00<00:00, 133449.06it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 52, 'dropout': 0.2, 'regulate_dot_product': False, 'normalize_embedding': False, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/100\n",
            "6/6 [==============================] - 1s 36ms/step - loss: 0.6984 - val_loss: 0.6265\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.6201 - val_loss: 0.6046\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.6076 - val_loss: 0.5943\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5985 - val_loss: 0.5893\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5945 - val_loss: 0.5880\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5910 - val_loss: 0.5838\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5875 - val_loss: 0.5810\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5841 - val_loss: 0.5808\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5827 - val_loss: 0.5803\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5802 - val_loss: 0.5752\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5791 - val_loss: 0.5729\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5782 - val_loss: 0.5735\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5765 - val_loss: 0.5716\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5750 - val_loss: 0.5706\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5736 - val_loss: 0.5690\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.5710 - val_loss: 0.5667\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5689 - val_loss: 0.5638\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5675 - val_loss: 0.5637\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5665 - val_loss: 0.5652\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5650 - val_loss: 0.5627\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5627 - val_loss: 0.5588\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5618 - val_loss: 0.5591\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5607 - val_loss: 0.5556\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5583 - val_loss: 0.5545\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5570 - val_loss: 0.5536\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5561 - val_loss: 0.5528\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5561 - val_loss: 0.5526\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5555 - val_loss: 0.5509\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5551 - val_loss: 0.5517\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5548 - val_loss: 0.5508\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5547 - val_loss: 0.5519\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5544 - val_loss: 0.5506\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5540 - val_loss: 0.5503\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5533 - val_loss: 0.5515\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5527 - val_loss: 0.5491\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5528 - val_loss: 0.5499\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5515 - val_loss: 0.5481\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5494 - val_loss: 0.5452\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5496 - val_loss: 0.5456\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5495 - val_loss: 0.5462\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5489 - val_loss: 0.5453\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5476 - val_loss: 0.5448\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5476 - val_loss: 0.5447\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5468 - val_loss: 0.5443\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5468 - val_loss: 0.5445\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5465 - val_loss: 0.5431\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5464 - val_loss: 0.5430\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5456 - val_loss: 0.5407\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5449 - val_loss: 0.5402\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5435 - val_loss: 0.5382\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5421 - val_loss: 0.5377\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5412 - val_loss: 0.5366\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5409 - val_loss: 0.5362\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5403 - val_loss: 0.5360\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5399 - val_loss: 0.5351\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5339\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5394 - val_loss: 0.5353\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5389 - val_loss: 0.5336\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5365\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5388 - val_loss: 0.5341\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5388 - val_loss: 0.5352\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5389 - val_loss: 0.5344\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5337\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5390 - val_loss: 0.5341\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5344\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5346\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5338\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5337\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5388 - val_loss: 0.5345\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5345\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5388 - val_loss: 0.5344\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5385 - val_loss: 0.5345\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5388 - val_loss: 0.5343\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5388 - val_loss: 0.5343\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5338\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5337\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5346\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5337\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5344\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5335\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5354\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5345\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5379 - val_loss: 0.5338\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5339\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5342\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5339\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5382 - val_loss: 0.5349\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5344\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5389 - val_loss: 0.5346\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5388 - val_loss: 0.5345\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5389 - val_loss: 0.5337\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5361\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5391 - val_loss: 0.5337\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5346\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5335\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5338\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5349\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5387 - val_loss: 0.5339\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5342\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5339\n",
            "test auc = 0.8324540089578976 in cv = 2\n",
            "\n",
            " cv 2 finished for synthetic5\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 3 starts for synthetic5\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/synthetic5-cv-train-3.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/synthetic5-cv-test-3.csv is loaded\n",
            "input dataset is already a feature datasetskipping the encoding and feature extraction step\n",
            "100% 3200/3200 [00:00<00:00, 118044.46it/s]\n",
            "100% 800/800 [00:00<00:00, 143579.08it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 52, 'dropout': 0.2, 'regulate_dot_product': False, 'normalize_embedding': False, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/100\n",
            "6/6 [==============================] - 1s 36ms/step - loss: 0.7198 - val_loss: 0.6408\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.6347 - val_loss: 0.6126\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.6179 - val_loss: 0.6028\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.6029 - val_loss: 0.5855\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5890 - val_loss: 0.5761\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5826 - val_loss: 0.5726\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5790 - val_loss: 0.5688\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5768 - val_loss: 0.5670\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5757 - val_loss: 0.5662\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5736 - val_loss: 0.5641\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5718 - val_loss: 0.5620\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5695 - val_loss: 0.5622\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5681 - val_loss: 0.5600\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5665 - val_loss: 0.5560\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5640 - val_loss: 0.5555\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5623 - val_loss: 0.5549\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5595 - val_loss: 0.5494\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5577 - val_loss: 0.5503\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5563 - val_loss: 0.5477\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5547 - val_loss: 0.5462\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5520 - val_loss: 0.5437\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5501 - val_loss: 0.5422\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5485 - val_loss: 0.5404\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5473 - val_loss: 0.5412\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5462 - val_loss: 0.5415\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5456 - val_loss: 0.5412\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5450 - val_loss: 0.5404\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5442 - val_loss: 0.5380\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5435 - val_loss: 0.5393\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5439 - val_loss: 0.5383\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5434 - val_loss: 0.5385\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5429 - val_loss: 0.5370\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5410 - val_loss: 0.5347\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5408 - val_loss: 0.5339\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5399 - val_loss: 0.5344\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5396 - val_loss: 0.5350\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5397 - val_loss: 0.5342\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5340\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5389 - val_loss: 0.5351\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5388 - val_loss: 0.5347\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5391 - val_loss: 0.5351\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5390 - val_loss: 0.5335\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5392 - val_loss: 0.5342\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5392 - val_loss: 0.5335\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5347\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5389 - val_loss: 0.5334\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5334\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5341\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5386 - val_loss: 0.5332\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5348\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5337\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5388 - val_loss: 0.5338\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5396 - val_loss: 0.5345\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5391 - val_loss: 0.5331\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5352\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5326\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5335\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5387 - val_loss: 0.5348\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5348\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5337\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5336\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5378 - val_loss: 0.5346\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5379 - val_loss: 0.5332\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5392 - val_loss: 0.5346\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5393 - val_loss: 0.5341\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5338\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5394 - val_loss: 0.5352\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5389 - val_loss: 0.5342\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5348\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5338\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5390 - val_loss: 0.5333\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5351\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5384 - val_loss: 0.5350\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5342\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5352\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5352\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5385 - val_loss: 0.5344\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5332\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5342\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5336\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5386 - val_loss: 0.5336\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5348\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5329\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5381 - val_loss: 0.5339\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5341\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5388 - val_loss: 0.5341\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5350\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5387 - val_loss: 0.5329\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5343\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5343\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5332\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5387 - val_loss: 0.5342\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5385 - val_loss: 0.5331\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5341\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5336\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5385 - val_loss: 0.5345\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5342\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5333\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5347\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5374 - val_loss: 0.5333\n",
            "test auc = 0.8310357459439357 in cv = 3\n",
            "\n",
            " cv 3 finished for synthetic5\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 4 starts for synthetic5\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/synthetic5-cv-train-4.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/synthetic5-cv-test-4.csv is loaded\n",
            "input dataset is already a feature datasetskipping the encoding and feature extraction step\n",
            "100% 3200/3200 [00:00<00:00, 112880.02it/s]\n",
            "100% 800/800 [00:00<00:00, 154244.88it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 52, 'dropout': 0.2, 'regulate_dot_product': False, 'normalize_embedding': False, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/100\n",
            "6/6 [==============================] - 1s 38ms/step - loss: 0.7284 - val_loss: 0.6435\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.6369 - val_loss: 0.6140\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.6189 - val_loss: 0.6027\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.6063 - val_loss: 0.5935\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5986 - val_loss: 0.5881\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5937 - val_loss: 0.5852\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5924 - val_loss: 0.5848\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5903 - val_loss: 0.5832\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5898 - val_loss: 0.5836\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5874 - val_loss: 0.5779\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5857 - val_loss: 0.5764\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5836 - val_loss: 0.5749\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5828 - val_loss: 0.5743\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5807 - val_loss: 0.5726\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5798 - val_loss: 0.5706\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5787 - val_loss: 0.5705\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5772 - val_loss: 0.5700\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5767 - val_loss: 0.5702\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5763 - val_loss: 0.5694\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5745 - val_loss: 0.5675\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5728 - val_loss: 0.5659\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5723 - val_loss: 0.5633\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5699 - val_loss: 0.5608\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5686 - val_loss: 0.5617\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5658 - val_loss: 0.5553\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5627 - val_loss: 0.5553\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5615 - val_loss: 0.5540\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5603 - val_loss: 0.5526\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5569 - val_loss: 0.5506\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5542 - val_loss: 0.5457\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5532 - val_loss: 0.5459\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5519 - val_loss: 0.5463\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5513 - val_loss: 0.5441\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5495 - val_loss: 0.5431\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5490 - val_loss: 0.5411\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5481 - val_loss: 0.5399\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5474 - val_loss: 0.5398\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5470 - val_loss: 0.5380\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5455 - val_loss: 0.5383\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5449 - val_loss: 0.5382\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5449 - val_loss: 0.5374\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5438 - val_loss: 0.5364\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5437 - val_loss: 0.5371\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5435 - val_loss: 0.5358\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5431 - val_loss: 0.5359\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5425 - val_loss: 0.5366\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5430 - val_loss: 0.5360\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5422 - val_loss: 0.5358\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5422 - val_loss: 0.5360\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5424 - val_loss: 0.5359\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5419 - val_loss: 0.5360\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5424 - val_loss: 0.5360\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.5423 - val_loss: 0.5362\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5416 - val_loss: 0.5366\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5416 - val_loss: 0.5358\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5411 - val_loss: 0.5349\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5411 - val_loss: 0.5346\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5407 - val_loss: 0.5338\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5405 - val_loss: 0.5319\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5405 - val_loss: 0.5328\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5398 - val_loss: 0.5333\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5393 - val_loss: 0.5315\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5393 - val_loss: 0.5318\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5399 - val_loss: 0.5327\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5389 - val_loss: 0.5315\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5312\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5390 - val_loss: 0.5319\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5393 - val_loss: 0.5314\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5315\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5315\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5380 - val_loss: 0.5322\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5325\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5327\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5310\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5321\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5305\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5323\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5311\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5377 - val_loss: 0.5315\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5313\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5380 - val_loss: 0.5324\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5375 - val_loss: 0.5317\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5316\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5382 - val_loss: 0.5316\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5317\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5312\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5318\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5383 - val_loss: 0.5318\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5326\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5382 - val_loss: 0.5326\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5379 - val_loss: 0.5316\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5386 - val_loss: 0.5329\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5382 - val_loss: 0.5314\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5381 - val_loss: 0.5330\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5314\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5318\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5387 - val_loss: 0.5315\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5384 - val_loss: 0.5326\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5385 - val_loss: 0.5323\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5388 - val_loss: 0.5320\n",
            "test auc = 0.8313440516364968 in cv = 4\n",
            "\n",
            " cv 4 finished for synthetic5\n",
            "\n",
            "average test auc for synthetic5 is 0.8321437639066784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tHfNzMdvnZD",
        "outputId": "3e702a34-8448-4869-e173-aef8f4505e03"
      },
      "source": [
        "!python main.py --dataset 'stat2011'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-27 00:32:15.278943: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "fit params are {'batch_size': 512, 'epochs': 200, 'validation_split': 0.1}\n",
            "optimizer and loss params = {'lr': 0.015, 'smoothing': 0.1}\n",
            "session cleared\n",
            "\n",
            " cv 0 starts for stat2011\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/stat2011-cv-train-0.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/stat2011-cv-test-0.csv is loaded\n",
            "dataset = stat2011\n",
            "num of attempts = 135.338 K\n",
            "num of students = 316\n",
            "num of items = 989\n",
            "input dataset is raw, will perform encode and extract features\n",
            "problem encoder is created\n",
            "train and test data are encoded\n",
            "train and test sequences are extracted\n",
            "100% 252/252 [00:00<00:00, 39689.26it/s]\n",
            "100% 64/64 [00:00<00:00, 39319.68it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "2021-07-27 00:32:18.212112: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-27 00:32:18.220249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:32:18.220873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-07-27 00:32:18.220908: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-27 00:32:18.223071: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-07-27 00:32:18.223143: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-07-27 00:32:18.224814: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-27 00:32:18.225141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-27 00:32:18.226564: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-27 00:32:18.227059: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-07-27 00:32:18.227233: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-07-27 00:32:18.227340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:32:18.228018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:32:18.228553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-07-27 00:32:18.228867: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-07-27 00:32:18.229153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:32:18.229778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-07-27 00:32:18.229852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:32:18.230488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:32:18.231127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-07-27 00:32:18.231179: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-27 00:32:18.724507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-27 00:32:18.724562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-07-27 00:32:18.724572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-07-27 00:32:18.724781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:32:18.725448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:32:18.726070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:32:18.726628: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-07-27 00:32:18.726670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14682 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 989, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "2021-07-27 00:32:18.783131: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-07-27 00:32:18.783577: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000165000 Hz\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "2021-07-27 00:32:20.039936: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-07-27 00:32:20.431891: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2/2 [==============================] - 2s 285ms/step - loss: 0.8166 - val_loss: 0.8078\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.8003 - val_loss: 0.7913\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7847 - val_loss: 0.7755\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7699 - val_loss: 0.7605\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7558 - val_loss: 0.7461\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.7423 - val_loss: 0.7325\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7295 - val_loss: 0.7193\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7172 - val_loss: 0.7069\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.7055 - val_loss: 0.6951\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6947 - val_loss: 0.6839\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6841 - val_loss: 0.6735\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6743 - val_loss: 0.6638\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6653 - val_loss: 0.6547\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6568 - val_loss: 0.6462\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6490 - val_loss: 0.6385\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6416 - val_loss: 0.6313\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6350 - val_loss: 0.6247\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6287 - val_loss: 0.6185\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6229 - val_loss: 0.6128\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6175 - val_loss: 0.6076\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6125 - val_loss: 0.6026\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6078 - val_loss: 0.5980\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6034 - val_loss: 0.5937\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5994 - val_loss: 0.5897\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5954 - val_loss: 0.5859\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5915 - val_loss: 0.5823\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5880 - val_loss: 0.5790\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5847 - val_loss: 0.5758\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5817 - val_loss: 0.5728\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5786 - val_loss: 0.5699\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5757 - val_loss: 0.5672\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5731 - val_loss: 0.5646\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5706 - val_loss: 0.5621\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5681 - val_loss: 0.5597\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5657 - val_loss: 0.5575\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5634 - val_loss: 0.5554\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5612 - val_loss: 0.5534\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5591 - val_loss: 0.5515\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5572 - val_loss: 0.5496\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5553 - val_loss: 0.5479\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5535 - val_loss: 0.5462\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5519 - val_loss: 0.5445\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5501 - val_loss: 0.5430\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5485 - val_loss: 0.5415\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5471 - val_loss: 0.5401\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5454 - val_loss: 0.5387\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5441 - val_loss: 0.5375\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5427 - val_loss: 0.5362\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5415 - val_loss: 0.5351\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5403 - val_loss: 0.5339\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5390 - val_loss: 0.5328\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5378 - val_loss: 0.5317\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5367 - val_loss: 0.5306\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5356 - val_loss: 0.5296\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5346 - val_loss: 0.5286\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5336 - val_loss: 0.5277\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5326 - val_loss: 0.5268\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5317 - val_loss: 0.5259\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5308 - val_loss: 0.5251\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5298 - val_loss: 0.5243\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5290 - val_loss: 0.5235\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5282 - val_loss: 0.5228\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5274 - val_loss: 0.5222\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5266 - val_loss: 0.5215\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5258 - val_loss: 0.5209\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5252 - val_loss: 0.5202\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5245 - val_loss: 0.5197\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5239 - val_loss: 0.5191\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5233 - val_loss: 0.5185\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5226 - val_loss: 0.5180\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5220 - val_loss: 0.5175\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5215 - val_loss: 0.5169\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5208 - val_loss: 0.5164\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5202 - val_loss: 0.5159\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5196 - val_loss: 0.5153\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5192 - val_loss: 0.5148\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5188 - val_loss: 0.5143\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5182 - val_loss: 0.5138\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5176 - val_loss: 0.5133\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5171 - val_loss: 0.5129\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5167 - val_loss: 0.5125\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5163 - val_loss: 0.5122\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5162 - val_loss: 0.5118\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5154 - val_loss: 0.5115\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5152 - val_loss: 0.5112\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5146 - val_loss: 0.5109\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5143 - val_loss: 0.5106\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5139 - val_loss: 0.5103\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5136 - val_loss: 0.5100\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5131 - val_loss: 0.5098\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5129 - val_loss: 0.5095\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5125 - val_loss: 0.5092\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5121 - val_loss: 0.5089\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5118 - val_loss: 0.5087\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5115 - val_loss: 0.5084\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5112 - val_loss: 0.5082\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5108 - val_loss: 0.5079\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5106 - val_loss: 0.5077\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5103 - val_loss: 0.5075\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5100 - val_loss: 0.5072\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5098 - val_loss: 0.5070\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5095 - val_loss: 0.5068\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5092 - val_loss: 0.5066\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5090 - val_loss: 0.5064\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5087 - val_loss: 0.5062\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5084 - val_loss: 0.5060\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5082 - val_loss: 0.5058\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5081 - val_loss: 0.5056\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5078 - val_loss: 0.5053\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5076 - val_loss: 0.5051\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5073 - val_loss: 0.5049\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5071 - val_loss: 0.5048\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5069 - val_loss: 0.5046\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5067 - val_loss: 0.5045\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5064 - val_loss: 0.5043\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5062 - val_loss: 0.5042\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5060 - val_loss: 0.5040\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5059 - val_loss: 0.5039\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5057 - val_loss: 0.5038\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5054 - val_loss: 0.5036\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5052 - val_loss: 0.5035\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5049 - val_loss: 0.5033\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5048 - val_loss: 0.5031\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5046 - val_loss: 0.5030\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5045 - val_loss: 0.5028\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5043 - val_loss: 0.5027\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5041 - val_loss: 0.5026\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5039 - val_loss: 0.5025\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5036 - val_loss: 0.5023\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5037 - val_loss: 0.5022\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5034 - val_loss: 0.5021\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5034 - val_loss: 0.5020\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5032 - val_loss: 0.5019\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5030 - val_loss: 0.5018\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5028 - val_loss: 0.5017\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5027 - val_loss: 0.5016\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5026 - val_loss: 0.5015\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5025 - val_loss: 0.5014\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5023 - val_loss: 0.5013\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5021 - val_loss: 0.5012\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5020 - val_loss: 0.5011\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5017 - val_loss: 0.5010\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5017 - val_loss: 0.5009\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5016 - val_loss: 0.5009\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5016 - val_loss: 0.5008\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5014 - val_loss: 0.5008\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5012 - val_loss: 0.5007\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5012 - val_loss: 0.5007\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5010 - val_loss: 0.5006\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5009 - val_loss: 0.5005\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5007 - val_loss: 0.5005\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5005 - val_loss: 0.5004\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5004 - val_loss: 0.5003\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5004 - val_loss: 0.5002\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5003 - val_loss: 0.5001\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5002 - val_loss: 0.5000\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5001 - val_loss: 0.4999\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4999 - val_loss: 0.4999\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4999 - val_loss: 0.4999\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4998 - val_loss: 0.4999\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4997 - val_loss: 0.4998\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4994 - val_loss: 0.4998\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4995 - val_loss: 0.4998\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4992 - val_loss: 0.4997\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4992 - val_loss: 0.4997\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4991 - val_loss: 0.4996\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4990 - val_loss: 0.4996\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4989 - val_loss: 0.4995\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4989 - val_loss: 0.4995\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4987 - val_loss: 0.4994\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4986 - val_loss: 0.4994\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4985 - val_loss: 0.4993\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4983 - val_loss: 0.4992\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4983 - val_loss: 0.4992\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4983 - val_loss: 0.4991\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4982 - val_loss: 0.4991\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4981 - val_loss: 0.4990\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4980 - val_loss: 0.4989\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4978 - val_loss: 0.4989\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4978 - val_loss: 0.4988\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4977 - val_loss: 0.4988\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4975 - val_loss: 0.4988\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4975 - val_loss: 0.4988\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4975 - val_loss: 0.4987\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4974 - val_loss: 0.4987\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.4973 - val_loss: 0.4987\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4973 - val_loss: 0.4986\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4972 - val_loss: 0.4986\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4972 - val_loss: 0.4986\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4970 - val_loss: 0.4985\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4969 - val_loss: 0.4985\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4969 - val_loss: 0.4984\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4968 - val_loss: 0.4983\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4967 - val_loss: 0.4982\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4966 - val_loss: 0.4982\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4966 - val_loss: 0.4981\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4965 - val_loss: 0.4980\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4964 - val_loss: 0.4980\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4963 - val_loss: 0.4980\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4963 - val_loss: 0.4980\n",
            "test auc = 0.805871132756087 in cv = 0\n",
            "\n",
            " cv 0 finished for stat2011\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 1 starts for stat2011\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/stat2011-cv-train-1.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/stat2011-cv-test-1.csv is loaded\n",
            "input dataset is raw, will perform encode and extract features\n",
            "problem encoder is created\n",
            "train and test data are encoded\n",
            "train and test sequences are extracted\n",
            "100% 253/253 [00:00<00:00, 40001.47it/s]\n",
            "100% 63/63 [00:00<00:00, 70220.87it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 989, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 169ms/step - loss: 0.8080 - val_loss: 0.7929\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7917 - val_loss: 0.7768\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7760 - val_loss: 0.7613\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.7610 - val_loss: 0.7465\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7467 - val_loss: 0.7324\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7328 - val_loss: 0.7189\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7197 - val_loss: 0.7061\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7071 - val_loss: 0.6938\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6951 - val_loss: 0.6823\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6839 - val_loss: 0.6713\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6733 - val_loss: 0.6610\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.6632 - val_loss: 0.6515\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6538 - val_loss: 0.6426\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6453 - val_loss: 0.6343\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6372 - val_loss: 0.6267\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6299 - val_loss: 0.6197\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6230 - val_loss: 0.6132\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6167 - val_loss: 0.6072\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6107 - val_loss: 0.6017\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6053 - val_loss: 0.5965\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6004 - val_loss: 0.5917\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5955 - val_loss: 0.5873\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5909 - val_loss: 0.5831\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5868 - val_loss: 0.5791\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5826 - val_loss: 0.5754\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5791 - val_loss: 0.5720\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5755 - val_loss: 0.5687\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5723 - val_loss: 0.5657\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5691 - val_loss: 0.5628\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5661 - val_loss: 0.5601\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5633 - val_loss: 0.5575\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5605 - val_loss: 0.5550\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5581 - val_loss: 0.5527\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5557 - val_loss: 0.5505\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5534 - val_loss: 0.5485\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5512 - val_loss: 0.5465\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5490 - val_loss: 0.5446\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5470 - val_loss: 0.5428\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5453 - val_loss: 0.5411\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5433 - val_loss: 0.5395\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5417 - val_loss: 0.5380\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5399 - val_loss: 0.5365\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5383 - val_loss: 0.5351\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5368 - val_loss: 0.5338\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5354 - val_loss: 0.5326\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5340 - val_loss: 0.5314\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5327 - val_loss: 0.5303\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5314 - val_loss: 0.5292\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5301 - val_loss: 0.5281\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5290 - val_loss: 0.5271\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5279 - val_loss: 0.5261\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5268 - val_loss: 0.5252\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5257 - val_loss: 0.5243\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5247 - val_loss: 0.5234\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5237 - val_loss: 0.5226\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5228 - val_loss: 0.5219\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5219 - val_loss: 0.5211\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5210 - val_loss: 0.5204\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5202 - val_loss: 0.5198\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5194 - val_loss: 0.5191\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5187 - val_loss: 0.5185\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5180 - val_loss: 0.5179\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5173 - val_loss: 0.5174\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5165 - val_loss: 0.5168\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5159 - val_loss: 0.5163\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5152 - val_loss: 0.5158\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5146 - val_loss: 0.5153\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5140 - val_loss: 0.5149\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5135 - val_loss: 0.5145\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5129 - val_loss: 0.5140\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5123 - val_loss: 0.5136\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5118 - val_loss: 0.5133\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5114 - val_loss: 0.5129\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5109 - val_loss: 0.5125\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5102 - val_loss: 0.5122\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5097 - val_loss: 0.5118\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5094 - val_loss: 0.5115\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5088 - val_loss: 0.5112\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5086 - val_loss: 0.5109\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5081 - val_loss: 0.5105\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5077 - val_loss: 0.5102\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5073 - val_loss: 0.5099\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5069 - val_loss: 0.5096\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5065 - val_loss: 0.5094\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5062 - val_loss: 0.5091\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5059 - val_loss: 0.5088\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5054 - val_loss: 0.5086\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5052 - val_loss: 0.5084\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5048 - val_loss: 0.5081\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5046 - val_loss: 0.5079\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5042 - val_loss: 0.5077\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5039 - val_loss: 0.5075\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5036 - val_loss: 0.5074\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5034 - val_loss: 0.5072\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5031 - val_loss: 0.5070\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5027 - val_loss: 0.5069\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5025 - val_loss: 0.5067\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5022 - val_loss: 0.5065\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5019 - val_loss: 0.5064\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5017 - val_loss: 0.5062\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5015 - val_loss: 0.5061\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5013 - val_loss: 0.5059\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5010 - val_loss: 0.5058\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5008 - val_loss: 0.5056\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5005 - val_loss: 0.5055\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5003 - val_loss: 0.5053\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5001 - val_loss: 0.5052\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4999 - val_loss: 0.5051\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4996 - val_loss: 0.5050\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4994 - val_loss: 0.5049\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4993 - val_loss: 0.5048\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4992 - val_loss: 0.5047\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4989 - val_loss: 0.5046\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4988 - val_loss: 0.5046\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4985 - val_loss: 0.5045\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4983 - val_loss: 0.5045\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4980 - val_loss: 0.5045\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4979 - val_loss: 0.5044\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4977 - val_loss: 0.5044\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4976 - val_loss: 0.5043\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4975 - val_loss: 0.5042\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4971 - val_loss: 0.5042\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4970 - val_loss: 0.5041\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4970 - val_loss: 0.5041\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4968 - val_loss: 0.5040\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4966 - val_loss: 0.5039\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4964 - val_loss: 0.5039\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4962 - val_loss: 0.5038\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4962 - val_loss: 0.5037\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4958 - val_loss: 0.5037\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4960 - val_loss: 0.5036\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4956 - val_loss: 0.5035\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4955 - val_loss: 0.5035\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4954 - val_loss: 0.5034\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4952 - val_loss: 0.5034\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4952 - val_loss: 0.5034\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4951 - val_loss: 0.5033\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4950 - val_loss: 0.5033\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4947 - val_loss: 0.5033\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4945 - val_loss: 0.5032\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4944 - val_loss: 0.5032\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4942 - val_loss: 0.5031\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4941 - val_loss: 0.5030\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4940 - val_loss: 0.5029\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4938 - val_loss: 0.5029\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4938 - val_loss: 0.5028\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4936 - val_loss: 0.5028\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4935 - val_loss: 0.5027\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4933 - val_loss: 0.5027\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4933 - val_loss: 0.5026\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4930 - val_loss: 0.5026\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4930 - val_loss: 0.5026\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4928 - val_loss: 0.5025\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4926 - val_loss: 0.5025\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4926 - val_loss: 0.5025\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4926 - val_loss: 0.5025\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4924 - val_loss: 0.5024\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4923 - val_loss: 0.5024\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4922 - val_loss: 0.5024\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4921 - val_loss: 0.5024\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4921 - val_loss: 0.5024\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4919 - val_loss: 0.5024\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4918 - val_loss: 0.5024\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4916 - val_loss: 0.5024\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4915 - val_loss: 0.5023\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4915 - val_loss: 0.5023\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4913 - val_loss: 0.5023\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4914 - val_loss: 0.5023\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4911 - val_loss: 0.5022\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4911 - val_loss: 0.5022\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4909 - val_loss: 0.5022\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4909 - val_loss: 0.5021\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4909 - val_loss: 0.5021\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4907 - val_loss: 0.5021\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4905 - val_loss: 0.5021\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4905 - val_loss: 0.5021\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4904 - val_loss: 0.5021\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4902 - val_loss: 0.5021\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4902 - val_loss: 0.5021\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4901 - val_loss: 0.5021\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4901 - val_loss: 0.5021\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4901 - val_loss: 0.5020\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4899 - val_loss: 0.5020\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4898 - val_loss: 0.5020\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4897 - val_loss: 0.5020\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4896 - val_loss: 0.5020\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4897 - val_loss: 0.5020\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4894 - val_loss: 0.5020\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4895 - val_loss: 0.5020\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4894 - val_loss: 0.5019\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4893 - val_loss: 0.5019\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4892 - val_loss: 0.5019\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4892 - val_loss: 0.5019\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4890 - val_loss: 0.5019\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4889 - val_loss: 0.5019\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4889 - val_loss: 0.5019\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4889 - val_loss: 0.5019\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4887 - val_loss: 0.5019\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4887 - val_loss: 0.5019\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4886 - val_loss: 0.5019\n",
            "test auc = 0.8024376015195501 in cv = 1\n",
            "\n",
            " cv 1 finished for stat2011\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 2 starts for stat2011\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/stat2011-cv-train-2.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/stat2011-cv-test-2.csv is loaded\n",
            "input dataset is raw, will perform encode and extract features\n",
            "problem encoder is created\n",
            "train and test data are encoded\n",
            "train and test sequences are extracted\n",
            "100% 253/253 [00:00<00:00, 26576.15it/s]\n",
            "100% 63/63 [00:00<00:00, 64167.35it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 989, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 163ms/step - loss: 0.8165 - val_loss: 0.8015\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.8002 - val_loss: 0.7853\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.7845 - val_loss: 0.7697\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.7695 - val_loss: 0.7547\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7551 - val_loss: 0.7405\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.7414 - val_loss: 0.7268\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7284 - val_loss: 0.7138\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.7159 - val_loss: 0.7015\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.7040 - val_loss: 0.6898\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6931 - val_loss: 0.6788\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6824 - val_loss: 0.6684\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6726 - val_loss: 0.6587\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6633 - val_loss: 0.6497\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6547 - val_loss: 0.6413\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6469 - val_loss: 0.6336\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6394 - val_loss: 0.6263\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6325 - val_loss: 0.6197\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6260 - val_loss: 0.6135\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6202 - val_loss: 0.6078\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6147 - val_loss: 0.6025\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6093 - val_loss: 0.5975\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6044 - val_loss: 0.5929\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6001 - val_loss: 0.5886\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5957 - val_loss: 0.5846\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5918 - val_loss: 0.5808\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5879 - val_loss: 0.5773\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5845 - val_loss: 0.5740\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5809 - val_loss: 0.5709\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5776 - val_loss: 0.5679\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5746 - val_loss: 0.5651\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.5716 - val_loss: 0.5625\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5688 - val_loss: 0.5600\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5663 - val_loss: 0.5576\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5637 - val_loss: 0.5553\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5613 - val_loss: 0.5532\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5590 - val_loss: 0.5511\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5570 - val_loss: 0.5492\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5549 - val_loss: 0.5473\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5528 - val_loss: 0.5454\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5509 - val_loss: 0.5437\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5491 - val_loss: 0.5420\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5474 - val_loss: 0.5404\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5456 - val_loss: 0.5389\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5439 - val_loss: 0.5375\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5426 - val_loss: 0.5361\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5408 - val_loss: 0.5349\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5395 - val_loss: 0.5336\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5383 - val_loss: 0.5325\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5370 - val_loss: 0.5314\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5357 - val_loss: 0.5304\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5346 - val_loss: 0.5293\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5333 - val_loss: 0.5283\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5323 - val_loss: 0.5274\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5311 - val_loss: 0.5264\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5300 - val_loss: 0.5255\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5292 - val_loss: 0.5246\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5282 - val_loss: 0.5238\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.5273 - val_loss: 0.5230\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5265 - val_loss: 0.5222\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.5256 - val_loss: 0.5214\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5247 - val_loss: 0.5207\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5239 - val_loss: 0.5200\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5232 - val_loss: 0.5194\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5224 - val_loss: 0.5188\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5217 - val_loss: 0.5182\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5211 - val_loss: 0.5177\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5204 - val_loss: 0.5172\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5197 - val_loss: 0.5167\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5191 - val_loss: 0.5162\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5184 - val_loss: 0.5157\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5178 - val_loss: 0.5153\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5173 - val_loss: 0.5148\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5167 - val_loss: 0.5144\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5163 - val_loss: 0.5139\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5155 - val_loss: 0.5135\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5151 - val_loss: 0.5131\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5147 - val_loss: 0.5127\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5142 - val_loss: 0.5123\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5139 - val_loss: 0.5119\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5133 - val_loss: 0.5116\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5128 - val_loss: 0.5112\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5125 - val_loss: 0.5109\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5120 - val_loss: 0.5106\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5118 - val_loss: 0.5103\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5112 - val_loss: 0.5100\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5108 - val_loss: 0.5097\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5107 - val_loss: 0.5094\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5102 - val_loss: 0.5091\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5098 - val_loss: 0.5089\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5095 - val_loss: 0.5087\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5092 - val_loss: 0.5084\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5090 - val_loss: 0.5082\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5086 - val_loss: 0.5079\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5082 - val_loss: 0.5077\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5079 - val_loss: 0.5075\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5076 - val_loss: 0.5073\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5073 - val_loss: 0.5071\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5070 - val_loss: 0.5069\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5067 - val_loss: 0.5068\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5065 - val_loss: 0.5066\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5062 - val_loss: 0.5064\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5059 - val_loss: 0.5062\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5057 - val_loss: 0.5061\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5055 - val_loss: 0.5059\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5053 - val_loss: 0.5058\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5051 - val_loss: 0.5056\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5047 - val_loss: 0.5055\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5045 - val_loss: 0.5054\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5044 - val_loss: 0.5053\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5042 - val_loss: 0.5052\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5039 - val_loss: 0.5050\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5037 - val_loss: 0.5049\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5035 - val_loss: 0.5047\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.5034 - val_loss: 0.5046\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5031 - val_loss: 0.5045\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5030 - val_loss: 0.5043\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5027 - val_loss: 0.5042\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5026 - val_loss: 0.5041\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5024 - val_loss: 0.5040\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5022 - val_loss: 0.5038\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5020 - val_loss: 0.5037\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5018 - val_loss: 0.5036\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5017 - val_loss: 0.5035\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5015 - val_loss: 0.5033\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5014 - val_loss: 0.5032\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5012 - val_loss: 0.5031\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5010 - val_loss: 0.5030\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5008 - val_loss: 0.5029\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5007 - val_loss: 0.5028\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5005 - val_loss: 0.5027\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5004 - val_loss: 0.5026\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5003 - val_loss: 0.5025\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5002 - val_loss: 0.5024\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4998 - val_loss: 0.5023\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4998 - val_loss: 0.5022\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4996 - val_loss: 0.5022\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4995 - val_loss: 0.5021\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4993 - val_loss: 0.5020\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4992 - val_loss: 0.5020\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4990 - val_loss: 0.5019\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4990 - val_loss: 0.5018\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4988 - val_loss: 0.5017\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4986 - val_loss: 0.5016\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4988 - val_loss: 0.5016\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4986 - val_loss: 0.5015\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4984 - val_loss: 0.5015\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4982 - val_loss: 0.5015\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4980 - val_loss: 0.5015\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4979 - val_loss: 0.5015\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4978 - val_loss: 0.5015\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4977 - val_loss: 0.5015\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4976 - val_loss: 0.5015\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4974 - val_loss: 0.5015\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4973 - val_loss: 0.5014\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4973 - val_loss: 0.5014\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4971 - val_loss: 0.5013\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4969 - val_loss: 0.5012\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4969 - val_loss: 0.5011\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4968 - val_loss: 0.5010\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4967 - val_loss: 0.5009\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4966 - val_loss: 0.5008\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4965 - val_loss: 0.5007\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4964 - val_loss: 0.5007\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4963 - val_loss: 0.5006\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4962 - val_loss: 0.5006\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4961 - val_loss: 0.5005\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4959 - val_loss: 0.5005\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4958 - val_loss: 0.5005\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4957 - val_loss: 0.5005\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4956 - val_loss: 0.5005\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4955 - val_loss: 0.5005\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4955 - val_loss: 0.5005\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4953 - val_loss: 0.5005\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4953 - val_loss: 0.5005\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4952 - val_loss: 0.5005\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4950 - val_loss: 0.5004\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4950 - val_loss: 0.5004\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4949 - val_loss: 0.5003\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4948 - val_loss: 0.5003\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4947 - val_loss: 0.5002\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4946 - val_loss: 0.5001\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4944 - val_loss: 0.5000\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.4944 - val_loss: 0.5000\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4943 - val_loss: 0.4999\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4943 - val_loss: 0.4999\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4942 - val_loss: 0.4998\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4940 - val_loss: 0.4998\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4939 - val_loss: 0.4998\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4939 - val_loss: 0.4998\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4937 - val_loss: 0.4997\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4938 - val_loss: 0.4997\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4936 - val_loss: 0.4996\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4936 - val_loss: 0.4996\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.4934 - val_loss: 0.4996\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4933 - val_loss: 0.4995\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4934 - val_loss: 0.4995\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4933 - val_loss: 0.4995\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4933 - val_loss: 0.4995\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4931 - val_loss: 0.4995\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4931 - val_loss: 0.4995\n",
            "test auc = 0.8052206638828371 in cv = 2\n",
            "\n",
            " cv 2 finished for stat2011\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 3 starts for stat2011\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/stat2011-cv-train-3.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/stat2011-cv-test-3.csv is loaded\n",
            "input dataset is raw, will perform encode and extract features\n",
            "problem encoder is created\n",
            "train and test data are encoded\n",
            "train and test sequences are extracted\n",
            "100% 253/253 [00:00<00:00, 39523.22it/s]\n",
            "100% 63/63 [00:00<00:00, 70183.57it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 989, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 158ms/step - loss: 0.7907 - val_loss: 0.7768\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7751 - val_loss: 0.7618\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.7599 - val_loss: 0.7476\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.7457 - val_loss: 0.7342\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7322 - val_loss: 0.7216\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.7194 - val_loss: 0.7096\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7073 - val_loss: 0.6983\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6959 - val_loss: 0.6876\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6850 - val_loss: 0.6774\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6748 - val_loss: 0.6679\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6653 - val_loss: 0.6590\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6562 - val_loss: 0.6507\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6479 - val_loss: 0.6430\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6400 - val_loss: 0.6358\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6326 - val_loss: 0.6291\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6257 - val_loss: 0.6229\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6195 - val_loss: 0.6171\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6136 - val_loss: 0.6117\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6080 - val_loss: 0.6066\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.6028 - val_loss: 0.6019\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5981 - val_loss: 0.5975\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5935 - val_loss: 0.5934\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5892 - val_loss: 0.5895\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5853 - val_loss: 0.5858\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5813 - val_loss: 0.5824\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5778 - val_loss: 0.5792\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5744 - val_loss: 0.5761\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5711 - val_loss: 0.5732\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5681 - val_loss: 0.5704\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5653 - val_loss: 0.5678\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5625 - val_loss: 0.5654\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5596 - val_loss: 0.5630\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5574 - val_loss: 0.5608\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.5548 - val_loss: 0.5587\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5526 - val_loss: 0.5567\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5503 - val_loss: 0.5548\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5482 - val_loss: 0.5530\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5461 - val_loss: 0.5513\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5444 - val_loss: 0.5496\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5427 - val_loss: 0.5481\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5408 - val_loss: 0.5466\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5392 - val_loss: 0.5451\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5375 - val_loss: 0.5438\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5360 - val_loss: 0.5425\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5345 - val_loss: 0.5412\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5331 - val_loss: 0.5400\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5317 - val_loss: 0.5389\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5303 - val_loss: 0.5378\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5292 - val_loss: 0.5368\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5280 - val_loss: 0.5358\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5268 - val_loss: 0.5348\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5259 - val_loss: 0.5339\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.5249 - val_loss: 0.5330\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5237 - val_loss: 0.5322\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5228 - val_loss: 0.5313\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5218 - val_loss: 0.5305\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5209 - val_loss: 0.5298\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5200 - val_loss: 0.5290\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5191 - val_loss: 0.5284\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5182 - val_loss: 0.5277\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5175 - val_loss: 0.5271\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5169 - val_loss: 0.5265\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5161 - val_loss: 0.5259\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5155 - val_loss: 0.5253\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5146 - val_loss: 0.5248\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5140 - val_loss: 0.5243\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5134 - val_loss: 0.5238\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5128 - val_loss: 0.5234\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5122 - val_loss: 0.5230\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 201ms/step - loss: 0.5116 - val_loss: 0.5226\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5110 - val_loss: 0.5222\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5106 - val_loss: 0.5218\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5100 - val_loss: 0.5214\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5095 - val_loss: 0.5210\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5091 - val_loss: 0.5207\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5085 - val_loss: 0.5204\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5081 - val_loss: 0.5201\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5076 - val_loss: 0.5198\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5072 - val_loss: 0.5195\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5068 - val_loss: 0.5192\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5063 - val_loss: 0.5189\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5059 - val_loss: 0.5186\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5056 - val_loss: 0.5184\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.5052 - val_loss: 0.5181\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5049 - val_loss: 0.5179\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5045 - val_loss: 0.5176\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5041 - val_loss: 0.5174\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5038 - val_loss: 0.5172\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5034 - val_loss: 0.5170\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5031 - val_loss: 0.5168\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5028 - val_loss: 0.5165\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5025 - val_loss: 0.5163\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5023 - val_loss: 0.5161\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5020 - val_loss: 0.5160\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5017 - val_loss: 0.5158\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5014 - val_loss: 0.5156\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5010 - val_loss: 0.5154\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5008 - val_loss: 0.5152\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5006 - val_loss: 0.5150\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5003 - val_loss: 0.5149\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5001 - val_loss: 0.5147\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4999 - val_loss: 0.5146\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4996 - val_loss: 0.5145\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4994 - val_loss: 0.5144\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4992 - val_loss: 0.5143\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4989 - val_loss: 0.5141\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4988 - val_loss: 0.5140\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4986 - val_loss: 0.5139\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4983 - val_loss: 0.5137\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4981 - val_loss: 0.5136\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4978 - val_loss: 0.5135\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4977 - val_loss: 0.5134\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4975 - val_loss: 0.5133\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4972 - val_loss: 0.5132\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4972 - val_loss: 0.5131\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4970 - val_loss: 0.5131\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4968 - val_loss: 0.5130\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4966 - val_loss: 0.5129\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4964 - val_loss: 0.5129\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4963 - val_loss: 0.5128\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4960 - val_loss: 0.5128\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4959 - val_loss: 0.5128\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4958 - val_loss: 0.5127\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4956 - val_loss: 0.5126\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4955 - val_loss: 0.5126\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4952 - val_loss: 0.5126\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4951 - val_loss: 0.5125\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4949 - val_loss: 0.5124\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4947 - val_loss: 0.5124\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4947 - val_loss: 0.5123\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4945 - val_loss: 0.5123\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4944 - val_loss: 0.5122\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4943 - val_loss: 0.5122\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4941 - val_loss: 0.5122\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4940 - val_loss: 0.5121\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4939 - val_loss: 0.5121\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4937 - val_loss: 0.5121\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4936 - val_loss: 0.5120\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4935 - val_loss: 0.5120\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4934 - val_loss: 0.5119\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4934 - val_loss: 0.5119\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4931 - val_loss: 0.5118\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4930 - val_loss: 0.5118\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4928 - val_loss: 0.5117\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4928 - val_loss: 0.5116\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4926 - val_loss: 0.5116\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4924 - val_loss: 0.5115\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4925 - val_loss: 0.5115\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4922 - val_loss: 0.5114\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4921 - val_loss: 0.5114\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4921 - val_loss: 0.5113\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4919 - val_loss: 0.5113\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4919 - val_loss: 0.5113\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4918 - val_loss: 0.5113\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4916 - val_loss: 0.5113\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4915 - val_loss: 0.5113\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4914 - val_loss: 0.5113\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4914 - val_loss: 0.5113\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4913 - val_loss: 0.5112\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4912 - val_loss: 0.5112\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4909 - val_loss: 0.5112\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4910 - val_loss: 0.5112\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4909 - val_loss: 0.5112\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4908 - val_loss: 0.5112\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4907 - val_loss: 0.5112\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4905 - val_loss: 0.5112\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4905 - val_loss: 0.5112\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4905 - val_loss: 0.5112\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4904 - val_loss: 0.5112\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4903 - val_loss: 0.5112\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4902 - val_loss: 0.5112\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4900 - val_loss: 0.5112\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4900 - val_loss: 0.5111\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4899 - val_loss: 0.5111\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4898 - val_loss: 0.5111\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4898 - val_loss: 0.5111\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4896 - val_loss: 0.5111\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4896 - val_loss: 0.5110\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4897 - val_loss: 0.5110\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4894 - val_loss: 0.5110\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4894 - val_loss: 0.5109\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4893 - val_loss: 0.5109\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4892 - val_loss: 0.5108\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4891 - val_loss: 0.5108\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4891 - val_loss: 0.5108\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.4890 - val_loss: 0.5107\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4895 - val_loss: 0.5107\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4889 - val_loss: 0.5107\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4888 - val_loss: 0.5106\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4888 - val_loss: 0.5106\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4887 - val_loss: 0.5106\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4886 - val_loss: 0.5106\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.4886 - val_loss: 0.5106\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4885 - val_loss: 0.5107\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4884 - val_loss: 0.5107\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4883 - val_loss: 0.5107\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4883 - val_loss: 0.5107\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4884 - val_loss: 0.5107\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.4881 - val_loss: 0.5108\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4880 - val_loss: 0.5108\n",
            "test auc = 0.8023638595021462 in cv = 3\n",
            "\n",
            " cv 3 finished for stat2011\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 4 starts for stat2011\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/stat2011-cv-train-4.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/stat2011-cv-test-4.csv is loaded\n",
            "input dataset is raw, will perform encode and extract features\n",
            "problem encoder is created\n",
            "train and test data are encoded\n",
            "train and test sequences are extracted\n",
            "100% 253/253 [00:00<00:00, 38228.94it/s]\n",
            "100% 63/63 [00:00<00:00, 58798.65it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 989, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 173ms/step - loss: 0.8313 - val_loss: 0.8187\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.8138 - val_loss: 0.8018\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7971 - val_loss: 0.7856\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.7812 - val_loss: 0.7701\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.7659 - val_loss: 0.7554\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7512 - val_loss: 0.7413\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.7373 - val_loss: 0.7278\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.7240 - val_loss: 0.7150\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.7113 - val_loss: 0.7028\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.6993 - val_loss: 0.6914\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6878 - val_loss: 0.6806\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6774 - val_loss: 0.6704\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6674 - val_loss: 0.6610\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6581 - val_loss: 0.6522\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.6495 - val_loss: 0.6440\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6414 - val_loss: 0.6364\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6338 - val_loss: 0.6293\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6269 - val_loss: 0.6227\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6205 - val_loss: 0.6166\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6145 - val_loss: 0.6110\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6089 - val_loss: 0.6057\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6038 - val_loss: 0.6007\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.5989 - val_loss: 0.5961\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5943 - val_loss: 0.5918\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5900 - val_loss: 0.5878\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5861 - val_loss: 0.5840\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5823 - val_loss: 0.5804\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.5786 - val_loss: 0.5770\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5751 - val_loss: 0.5738\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5720 - val_loss: 0.5708\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5689 - val_loss: 0.5679\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5661 - val_loss: 0.5651\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.5632 - val_loss: 0.5625\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5606 - val_loss: 0.5601\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5581 - val_loss: 0.5577\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5557 - val_loss: 0.5554\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5534 - val_loss: 0.5533\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.5512 - val_loss: 0.5512\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5491 - val_loss: 0.5492\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5471 - val_loss: 0.5473\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5452 - val_loss: 0.5456\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5433 - val_loss: 0.5439\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5415 - val_loss: 0.5422\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5400 - val_loss: 0.5407\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5385 - val_loss: 0.5392\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5369 - val_loss: 0.5378\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5354 - val_loss: 0.5365\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5340 - val_loss: 0.5352\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.5327 - val_loss: 0.5340\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5314 - val_loss: 0.5328\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5301 - val_loss: 0.5317\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5291 - val_loss: 0.5306\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5278 - val_loss: 0.5296\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5266 - val_loss: 0.5286\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5257 - val_loss: 0.5277\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5245 - val_loss: 0.5268\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5235 - val_loss: 0.5259\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5226 - val_loss: 0.5250\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.5217 - val_loss: 0.5242\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5207 - val_loss: 0.5234\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5200 - val_loss: 0.5227\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5191 - val_loss: 0.5220\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5182 - val_loss: 0.5213\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5176 - val_loss: 0.5207\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5168 - val_loss: 0.5201\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5160 - val_loss: 0.5195\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.5152 - val_loss: 0.5189\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5146 - val_loss: 0.5183\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5138 - val_loss: 0.5178\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5133 - val_loss: 0.5173\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5127 - val_loss: 0.5168\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5121 - val_loss: 0.5163\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.5115 - val_loss: 0.5158\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5110 - val_loss: 0.5153\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5104 - val_loss: 0.5149\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5100 - val_loss: 0.5144\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5094 - val_loss: 0.5140\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5089 - val_loss: 0.5135\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5085 - val_loss: 0.5131\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5081 - val_loss: 0.5127\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5075 - val_loss: 0.5124\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5071 - val_loss: 0.5120\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5068 - val_loss: 0.5117\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5063 - val_loss: 0.5114\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5057 - val_loss: 0.5111\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5054 - val_loss: 0.5108\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5051 - val_loss: 0.5105\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5047 - val_loss: 0.5103\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5043 - val_loss: 0.5100\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5039 - val_loss: 0.5098\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5036 - val_loss: 0.5095\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5034 - val_loss: 0.5093\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5030 - val_loss: 0.5091\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5026 - val_loss: 0.5088\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5023 - val_loss: 0.5086\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5020 - val_loss: 0.5084\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5017 - val_loss: 0.5082\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.5014 - val_loss: 0.5080\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5011 - val_loss: 0.5078\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5009 - val_loss: 0.5076\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5006 - val_loss: 0.5074\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5004 - val_loss: 0.5072\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5001 - val_loss: 0.5070\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4998 - val_loss: 0.5067\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4996 - val_loss: 0.5065\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4992 - val_loss: 0.5064\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4991 - val_loss: 0.5062\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4988 - val_loss: 0.5060\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4986 - val_loss: 0.5058\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4985 - val_loss: 0.5057\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4982 - val_loss: 0.5055\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4979 - val_loss: 0.5054\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4978 - val_loss: 0.5053\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4976 - val_loss: 0.5052\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4975 - val_loss: 0.5051\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4971 - val_loss: 0.5050\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4968 - val_loss: 0.5049\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4969 - val_loss: 0.5048\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4966 - val_loss: 0.5047\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4963 - val_loss: 0.5046\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4963 - val_loss: 0.5045\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4959 - val_loss: 0.5044\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4957 - val_loss: 0.5043\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4956 - val_loss: 0.5042\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4955 - val_loss: 0.5041\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.4952 - val_loss: 0.5040\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4952 - val_loss: 0.5038\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4950 - val_loss: 0.5037\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4948 - val_loss: 0.5036\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4947 - val_loss: 0.5035\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4944 - val_loss: 0.5034\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4944 - val_loss: 0.5033\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4942 - val_loss: 0.5032\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4939 - val_loss: 0.5032\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4939 - val_loss: 0.5031\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4937 - val_loss: 0.5030\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4936 - val_loss: 0.5030\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4934 - val_loss: 0.5029\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4933 - val_loss: 0.5028\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4931 - val_loss: 0.5027\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4931 - val_loss: 0.5026\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.4929 - val_loss: 0.5025\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4927 - val_loss: 0.5024\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4927 - val_loss: 0.5024\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4925 - val_loss: 0.5023\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4923 - val_loss: 0.5023\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4923 - val_loss: 0.5022\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4920 - val_loss: 0.5022\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4920 - val_loss: 0.5022\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4918 - val_loss: 0.5022\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4918 - val_loss: 0.5021\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4917 - val_loss: 0.5021\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4916 - val_loss: 0.5021\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4914 - val_loss: 0.5020\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4912 - val_loss: 0.5020\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4911 - val_loss: 0.5020\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4910 - val_loss: 0.5020\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4909 - val_loss: 0.5019\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4907 - val_loss: 0.5019\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4907 - val_loss: 0.5019\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4905 - val_loss: 0.5018\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4905 - val_loss: 0.5018\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4903 - val_loss: 0.5017\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4902 - val_loss: 0.5017\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4901 - val_loss: 0.5016\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4901 - val_loss: 0.5016\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4900 - val_loss: 0.5015\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.4899 - val_loss: 0.5015\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4898 - val_loss: 0.5014\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4897 - val_loss: 0.5014\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4895 - val_loss: 0.5013\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4893 - val_loss: 0.5013\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4894 - val_loss: 0.5012\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4892 - val_loss: 0.5012\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4891 - val_loss: 0.5011\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4891 - val_loss: 0.5011\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4891 - val_loss: 0.5011\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4889 - val_loss: 0.5011\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.4888 - val_loss: 0.5011\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4887 - val_loss: 0.5011\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4886 - val_loss: 0.5011\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4885 - val_loss: 0.5011\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4884 - val_loss: 0.5011\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4883 - val_loss: 0.5011\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4883 - val_loss: 0.5010\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4882 - val_loss: 0.5010\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4880 - val_loss: 0.5009\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4881 - val_loss: 0.5009\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4880 - val_loss: 0.5009\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4877 - val_loss: 0.5009\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4875 - val_loss: 0.5009\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.4876 - val_loss: 0.5009\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.4876 - val_loss: 0.5008\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4875 - val_loss: 0.5008\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4873 - val_loss: 0.5007\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4874 - val_loss: 0.5006\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4873 - val_loss: 0.5006\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4872 - val_loss: 0.5005\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4871 - val_loss: 0.5005\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4870 - val_loss: 0.5005\n",
            "test auc = 0.802912005738056 in cv = 4\n",
            "\n",
            " cv 4 finished for stat2011\n",
            "\n",
            "average test auc for stat2011 is 0.8037610526797353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR9TVmPQ6o8d",
        "outputId": "74ea51af-ee83-47ba-950b-811791276a00"
      },
      "source": [
        "!python main.py --dataset 'assist2017'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-27 00:33:05.633121: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "fit params are {'batch_size': 512, 'epochs': 150, 'validation_split': 0.1}\n",
            "optimizer and loss params = {'lr': 0.015, 'smoothing': 0.1}\n",
            "session cleared\n",
            "\n",
            " cv 0 starts for assist2017\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/assist2017-cv-train-0.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/assist2017-cv-test-0.csv is loaded\n",
            "dataset = assist2017\n",
            "num of attempts = 393.663 K\n",
            "num of students = 1709\n",
            "num of items = 4119\n",
            "input dataset is raw, will perform encode and extract features\n",
            "problem encoder is created\n",
            "train and test data are encoded\n",
            "train and test sequences are extracted\n",
            "100% 1367/1367 [00:00<00:00, 40926.61it/s]\n",
            "100% 342/342 [00:00<00:00, 51690.10it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "2021-07-27 00:33:10.065396: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-27 00:33:10.074049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:33:10.074767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-07-27 00:33:10.074817: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-27 00:33:10.077217: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-07-27 00:33:10.077319: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-07-27 00:33:10.078951: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-27 00:33:10.079356: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-27 00:33:10.080991: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-27 00:33:10.081518: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-07-27 00:33:10.081756: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-07-27 00:33:10.081894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:33:10.082630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:33:10.083277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-07-27 00:33:10.083628: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-07-27 00:33:10.083932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:33:10.084584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-07-27 00:33:10.084687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:33:10.085362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:33:10.086001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-07-27 00:33:10.086059: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-27 00:33:10.608890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-27 00:33:10.608945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-07-27 00:33:10.608971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-07-27 00:33:10.609181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:33:10.609948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:33:10.610623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:33:10.611314: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-07-27 00:33:10.611368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14682 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 4119, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "2021-07-27 00:33:10.668360: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-07-27 00:33:10.668767: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000165000 Hz\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "2021-07-27 00:33:11.996650: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-07-27 00:33:12.403569: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "4/4 [==============================] - 2s 82ms/step - loss: 0.8037 - val_loss: 0.7908\n",
            "Epoch 2/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.7871 - val_loss: 0.7757\n",
            "Epoch 3/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.7714 - val_loss: 0.7613\n",
            "Epoch 4/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.7563 - val_loss: 0.7476\n",
            "Epoch 5/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.7420 - val_loss: 0.7345\n",
            "Epoch 6/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.7283 - val_loss: 0.7221\n",
            "Epoch 7/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.7154 - val_loss: 0.7104\n",
            "Epoch 8/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.7033 - val_loss: 0.6994\n",
            "Epoch 9/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6919 - val_loss: 0.6892\n",
            "Epoch 10/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6815 - val_loss: 0.6799\n",
            "Epoch 11/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6719 - val_loss: 0.6715\n",
            "Epoch 12/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6632 - val_loss: 0.6639\n",
            "Epoch 13/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.6553 - val_loss: 0.6570\n",
            "Epoch 14/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6482 - val_loss: 0.6508\n",
            "Epoch 15/150\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6417 - val_loss: 0.6453\n",
            "Epoch 16/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6359 - val_loss: 0.6402\n",
            "Epoch 17/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6306 - val_loss: 0.6356\n",
            "Epoch 18/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6255 - val_loss: 0.6315\n",
            "Epoch 19/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6211 - val_loss: 0.6277\n",
            "Epoch 20/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6170 - val_loss: 0.6243\n",
            "Epoch 21/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6132 - val_loss: 0.6212\n",
            "Epoch 22/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6096 - val_loss: 0.6183\n",
            "Epoch 23/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6065 - val_loss: 0.6157\n",
            "Epoch 24/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6035 - val_loss: 0.6133\n",
            "Epoch 25/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6007 - val_loss: 0.6110\n",
            "Epoch 26/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5982 - val_loss: 0.6090\n",
            "Epoch 27/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5957 - val_loss: 0.6071\n",
            "Epoch 28/150\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.5935 - val_loss: 0.6054\n",
            "Epoch 29/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5914 - val_loss: 0.6038\n",
            "Epoch 30/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5895 - val_loss: 0.6023\n",
            "Epoch 31/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5877 - val_loss: 0.6010\n",
            "Epoch 32/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5861 - val_loss: 0.5997\n",
            "Epoch 33/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5844 - val_loss: 0.5985\n",
            "Epoch 34/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5828 - val_loss: 0.5974\n",
            "Epoch 35/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5814 - val_loss: 0.5964\n",
            "Epoch 36/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5802 - val_loss: 0.5954\n",
            "Epoch 37/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5788 - val_loss: 0.5946\n",
            "Epoch 38/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5776 - val_loss: 0.5937\n",
            "Epoch 39/150\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5765 - val_loss: 0.5930\n",
            "Epoch 40/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5754 - val_loss: 0.5923\n",
            "Epoch 41/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5743 - val_loss: 0.5916\n",
            "Epoch 42/150\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5734 - val_loss: 0.5910\n",
            "Epoch 43/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5724 - val_loss: 0.5904\n",
            "Epoch 44/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5715 - val_loss: 0.5899\n",
            "Epoch 45/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5706 - val_loss: 0.5893\n",
            "Epoch 46/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5699 - val_loss: 0.5889\n",
            "Epoch 47/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5691 - val_loss: 0.5884\n",
            "Epoch 48/150\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5684 - val_loss: 0.5879\n",
            "Epoch 49/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5675 - val_loss: 0.5875\n",
            "Epoch 50/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5669 - val_loss: 0.5872\n",
            "Epoch 51/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5662 - val_loss: 0.5868\n",
            "Epoch 52/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5656 - val_loss: 0.5865\n",
            "Epoch 53/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5650 - val_loss: 0.5862\n",
            "Epoch 54/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5644 - val_loss: 0.5859\n",
            "Epoch 55/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5638 - val_loss: 0.5856\n",
            "Epoch 56/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5632 - val_loss: 0.5854\n",
            "Epoch 57/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5627 - val_loss: 0.5851\n",
            "Epoch 58/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5622 - val_loss: 0.5849\n",
            "Epoch 59/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5617 - val_loss: 0.5847\n",
            "Epoch 60/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5612 - val_loss: 0.5845\n",
            "Epoch 61/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5608 - val_loss: 0.5843\n",
            "Epoch 62/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5603 - val_loss: 0.5841\n",
            "Epoch 63/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5599 - val_loss: 0.5839\n",
            "Epoch 64/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5594 - val_loss: 0.5838\n",
            "Epoch 65/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5591 - val_loss: 0.5836\n",
            "Epoch 66/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5586 - val_loss: 0.5835\n",
            "Epoch 67/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5583 - val_loss: 0.5833\n",
            "Epoch 68/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5579 - val_loss: 0.5832\n",
            "Epoch 69/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5575 - val_loss: 0.5831\n",
            "Epoch 70/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5572 - val_loss: 0.5830\n",
            "Epoch 71/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5569 - val_loss: 0.5829\n",
            "Epoch 72/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5565 - val_loss: 0.5828\n",
            "Epoch 73/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5562 - val_loss: 0.5827\n",
            "Epoch 74/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5560 - val_loss: 0.5826\n",
            "Epoch 75/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5556 - val_loss: 0.5826\n",
            "Epoch 76/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5553 - val_loss: 0.5825\n",
            "Epoch 77/150\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5550 - val_loss: 0.5824\n",
            "Epoch 78/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5547 - val_loss: 0.5824\n",
            "Epoch 79/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5545 - val_loss: 0.5824\n",
            "Epoch 80/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5542 - val_loss: 0.5823\n",
            "Epoch 81/150\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5538 - val_loss: 0.5822\n",
            "Epoch 82/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5536 - val_loss: 0.5822\n",
            "Epoch 83/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5535 - val_loss: 0.5822\n",
            "Epoch 84/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5531 - val_loss: 0.5821\n",
            "Epoch 85/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5530 - val_loss: 0.5821\n",
            "Epoch 86/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5527 - val_loss: 0.5821\n",
            "Epoch 87/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5526 - val_loss: 0.5820\n",
            "Epoch 88/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5523 - val_loss: 0.5820\n",
            "Epoch 89/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5521 - val_loss: 0.5820\n",
            "Epoch 90/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5519 - val_loss: 0.5820\n",
            "Epoch 91/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5517 - val_loss: 0.5820\n",
            "Epoch 92/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5515 - val_loss: 0.5819\n",
            "Epoch 93/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5512 - val_loss: 0.5819\n",
            "Epoch 94/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5511 - val_loss: 0.5819\n",
            "Epoch 95/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5509 - val_loss: 0.5819\n",
            "Epoch 96/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5508 - val_loss: 0.5819\n",
            "Epoch 97/150\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5506 - val_loss: 0.5819\n",
            "Epoch 98/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5504 - val_loss: 0.5819\n",
            "Epoch 99/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5502 - val_loss: 0.5819\n",
            "Epoch 100/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5501 - val_loss: 0.5819\n",
            "Epoch 101/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5499 - val_loss: 0.5820\n",
            "Epoch 102/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5498 - val_loss: 0.5820\n",
            "Epoch 103/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5496 - val_loss: 0.5820\n",
            "Epoch 104/150\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.5495 - val_loss: 0.5820\n",
            "Epoch 105/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5492 - val_loss: 0.5820\n",
            "Epoch 106/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5492 - val_loss: 0.5820\n",
            "Epoch 107/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5490 - val_loss: 0.5820\n",
            "Epoch 108/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5489 - val_loss: 0.5820\n",
            "Epoch 109/150\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5487 - val_loss: 0.5821\n",
            "Epoch 110/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5485 - val_loss: 0.5821\n",
            "Epoch 111/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5484 - val_loss: 0.5821\n",
            "Epoch 112/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5483 - val_loss: 0.5821\n",
            "Epoch 113/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5482 - val_loss: 0.5821\n",
            "Epoch 114/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5480 - val_loss: 0.5821\n",
            "Epoch 115/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5479 - val_loss: 0.5822\n",
            "Epoch 116/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5477 - val_loss: 0.5822\n",
            "Epoch 117/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5477 - val_loss: 0.5822\n",
            "Epoch 118/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5474 - val_loss: 0.5822\n",
            "Epoch 119/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5473 - val_loss: 0.5823\n",
            "Epoch 120/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5472 - val_loss: 0.5823\n",
            "Epoch 121/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5471 - val_loss: 0.5823\n",
            "Epoch 122/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5470 - val_loss: 0.5823\n",
            "Epoch 123/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5469 - val_loss: 0.5824\n",
            "Epoch 124/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5467 - val_loss: 0.5824\n",
            "Epoch 125/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5467 - val_loss: 0.5824\n",
            "Epoch 126/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5465 - val_loss: 0.5825\n",
            "Epoch 127/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5464 - val_loss: 0.5825\n",
            "Epoch 128/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5462 - val_loss: 0.5826\n",
            "Epoch 129/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5462 - val_loss: 0.5826\n",
            "Epoch 130/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5461 - val_loss: 0.5826\n",
            "Epoch 131/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5460 - val_loss: 0.5826\n",
            "Epoch 132/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5458 - val_loss: 0.5827\n",
            "Epoch 133/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5458 - val_loss: 0.5827\n",
            "Epoch 134/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5456 - val_loss: 0.5827\n",
            "Epoch 135/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5455 - val_loss: 0.5828\n",
            "Epoch 136/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5453 - val_loss: 0.5828\n",
            "Epoch 137/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5453 - val_loss: 0.5829\n",
            "Epoch 138/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5451 - val_loss: 0.5829\n",
            "Epoch 139/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5451 - val_loss: 0.5830\n",
            "Epoch 140/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5450 - val_loss: 0.5830\n",
            "Epoch 141/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5448 - val_loss: 0.5831\n",
            "Epoch 142/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5448 - val_loss: 0.5831\n",
            "Epoch 143/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5447 - val_loss: 0.5831\n",
            "Epoch 144/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5445 - val_loss: 0.5832\n",
            "Epoch 145/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5446 - val_loss: 0.5833\n",
            "Epoch 146/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5444 - val_loss: 0.5833\n",
            "Epoch 147/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5443 - val_loss: 0.5834\n",
            "Epoch 148/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5442 - val_loss: 0.5834\n",
            "Epoch 149/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5441 - val_loss: 0.5835\n",
            "Epoch 150/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5440 - val_loss: 0.5836\n",
            "test auc = 0.7901678714140024 in cv = 0\n",
            "\n",
            " cv 0 finished for assist2017\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 1 starts for assist2017\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/assist2017-cv-train-1.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/assist2017-cv-test-1.csv is loaded\n",
            "input dataset is raw, will perform encode and extract features\n",
            "problem encoder is created\n",
            "train and test data are encoded\n",
            "train and test sequences are extracted\n",
            "100% 1367/1367 [00:00<00:00, 53427.89it/s]\n",
            "100% 342/342 [00:00<00:00, 50733.96it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 4119, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/150\n",
            "4/4 [==============================] - 1s 66ms/step - loss: 0.8081 - val_loss: 0.7956\n",
            "Epoch 2/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.7912 - val_loss: 0.7796\n",
            "Epoch 3/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.7753 - val_loss: 0.7644\n",
            "Epoch 4/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.7601 - val_loss: 0.7500\n",
            "Epoch 5/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.7459 - val_loss: 0.7365\n",
            "Epoch 6/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.7322 - val_loss: 0.7238\n",
            "Epoch 7/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.7193 - val_loss: 0.7119\n",
            "Epoch 8/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.7072 - val_loss: 0.7009\n",
            "Epoch 9/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.6960 - val_loss: 0.6906\n",
            "Epoch 10/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6856 - val_loss: 0.6812\n",
            "Epoch 11/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6759 - val_loss: 0.6725\n",
            "Epoch 12/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6671 - val_loss: 0.6646\n",
            "Epoch 13/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6590 - val_loss: 0.6575\n",
            "Epoch 14/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6516 - val_loss: 0.6510\n",
            "Epoch 15/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6450 - val_loss: 0.6451\n",
            "Epoch 16/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6389 - val_loss: 0.6397\n",
            "Epoch 17/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6334 - val_loss: 0.6349\n",
            "Epoch 18/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.6284 - val_loss: 0.6305\n",
            "Epoch 19/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.6238 - val_loss: 0.6265\n",
            "Epoch 20/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6197 - val_loss: 0.6229\n",
            "Epoch 21/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6157 - val_loss: 0.6195\n",
            "Epoch 22/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6123 - val_loss: 0.6165\n",
            "Epoch 23/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6089 - val_loss: 0.6137\n",
            "Epoch 24/150\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.6058 - val_loss: 0.6110\n",
            "Epoch 25/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6031 - val_loss: 0.6086\n",
            "Epoch 26/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6005 - val_loss: 0.6064\n",
            "Epoch 27/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5980 - val_loss: 0.6043\n",
            "Epoch 28/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5958 - val_loss: 0.6024\n",
            "Epoch 29/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5936 - val_loss: 0.6006\n",
            "Epoch 30/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5916 - val_loss: 0.5990\n",
            "Epoch 31/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5898 - val_loss: 0.5975\n",
            "Epoch 32/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5881 - val_loss: 0.5960\n",
            "Epoch 33/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5864 - val_loss: 0.5947\n",
            "Epoch 34/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5849 - val_loss: 0.5934\n",
            "Epoch 35/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5833 - val_loss: 0.5923\n",
            "Epoch 36/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5820 - val_loss: 0.5912\n",
            "Epoch 37/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5807 - val_loss: 0.5901\n",
            "Epoch 38/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5795 - val_loss: 0.5892\n",
            "Epoch 39/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5783 - val_loss: 0.5883\n",
            "Epoch 40/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5773 - val_loss: 0.5874\n",
            "Epoch 41/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5762 - val_loss: 0.5866\n",
            "Epoch 42/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5751 - val_loss: 0.5859\n",
            "Epoch 43/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5741 - val_loss: 0.5852\n",
            "Epoch 44/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5733 - val_loss: 0.5845\n",
            "Epoch 45/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5723 - val_loss: 0.5838\n",
            "Epoch 46/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5716 - val_loss: 0.5832\n",
            "Epoch 47/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5707 - val_loss: 0.5827\n",
            "Epoch 48/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5699 - val_loss: 0.5822\n",
            "Epoch 49/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5692 - val_loss: 0.5817\n",
            "Epoch 50/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5684 - val_loss: 0.5812\n",
            "Epoch 51/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5679 - val_loss: 0.5808\n",
            "Epoch 52/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5671 - val_loss: 0.5803\n",
            "Epoch 53/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5665 - val_loss: 0.5799\n",
            "Epoch 54/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5659 - val_loss: 0.5795\n",
            "Epoch 55/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5653 - val_loss: 0.5792\n",
            "Epoch 56/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5648 - val_loss: 0.5788\n",
            "Epoch 57/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5641 - val_loss: 0.5785\n",
            "Epoch 58/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5637 - val_loss: 0.5782\n",
            "Epoch 59/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5632 - val_loss: 0.5779\n",
            "Epoch 60/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5626 - val_loss: 0.5776\n",
            "Epoch 61/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5622 - val_loss: 0.5773\n",
            "Epoch 62/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5617 - val_loss: 0.5771\n",
            "Epoch 63/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5613 - val_loss: 0.5768\n",
            "Epoch 64/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5609 - val_loss: 0.5766\n",
            "Epoch 65/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5604 - val_loss: 0.5764\n",
            "Epoch 66/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5600 - val_loss: 0.5762\n",
            "Epoch 67/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5597 - val_loss: 0.5760\n",
            "Epoch 68/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5592 - val_loss: 0.5758\n",
            "Epoch 69/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5589 - val_loss: 0.5756\n",
            "Epoch 70/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5585 - val_loss: 0.5754\n",
            "Epoch 71/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5581 - val_loss: 0.5753\n",
            "Epoch 72/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5578 - val_loss: 0.5751\n",
            "Epoch 73/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5574 - val_loss: 0.5749\n",
            "Epoch 74/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5572 - val_loss: 0.5748\n",
            "Epoch 75/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5569 - val_loss: 0.5746\n",
            "Epoch 76/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5566 - val_loss: 0.5745\n",
            "Epoch 77/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5563 - val_loss: 0.5744\n",
            "Epoch 78/150\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5559 - val_loss: 0.5743\n",
            "Epoch 79/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5557 - val_loss: 0.5742\n",
            "Epoch 80/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5554 - val_loss: 0.5741\n",
            "Epoch 81/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5551 - val_loss: 0.5740\n",
            "Epoch 82/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5548 - val_loss: 0.5739\n",
            "Epoch 83/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5546 - val_loss: 0.5738\n",
            "Epoch 84/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5544 - val_loss: 0.5737\n",
            "Epoch 85/150\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5542 - val_loss: 0.5736\n",
            "Epoch 86/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5539 - val_loss: 0.5735\n",
            "Epoch 87/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5537 - val_loss: 0.5735\n",
            "Epoch 88/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5535 - val_loss: 0.5734\n",
            "Epoch 89/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5533 - val_loss: 0.5734\n",
            "Epoch 90/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5530 - val_loss: 0.5733\n",
            "Epoch 91/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5528 - val_loss: 0.5733\n",
            "Epoch 92/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5526 - val_loss: 0.5732\n",
            "Epoch 93/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5525 - val_loss: 0.5732\n",
            "Epoch 94/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5523 - val_loss: 0.5731\n",
            "Epoch 95/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5521 - val_loss: 0.5731\n",
            "Epoch 96/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5519 - val_loss: 0.5731\n",
            "Epoch 97/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5517 - val_loss: 0.5731\n",
            "Epoch 98/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5515 - val_loss: 0.5731\n",
            "Epoch 99/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5514 - val_loss: 0.5730\n",
            "Epoch 100/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5512 - val_loss: 0.5730\n",
            "Epoch 101/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5511 - val_loss: 0.5730\n",
            "Epoch 102/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5509 - val_loss: 0.5730\n",
            "Epoch 103/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5507 - val_loss: 0.5730\n",
            "Epoch 104/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5505 - val_loss: 0.5730\n",
            "Epoch 105/150\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5503 - val_loss: 0.5730\n",
            "Epoch 106/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5502 - val_loss: 0.5729\n",
            "Epoch 107/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5501 - val_loss: 0.5729\n",
            "Epoch 108/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5500 - val_loss: 0.5729\n",
            "Epoch 109/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5497 - val_loss: 0.5729\n",
            "Epoch 110/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5496 - val_loss: 0.5730\n",
            "Epoch 111/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5495 - val_loss: 0.5730\n",
            "Epoch 112/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5495 - val_loss: 0.5730\n",
            "Epoch 113/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5492 - val_loss: 0.5730\n",
            "Epoch 114/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5492 - val_loss: 0.5730\n",
            "Epoch 115/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5489 - val_loss: 0.5730\n",
            "Epoch 116/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5489 - val_loss: 0.5730\n",
            "Epoch 117/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5487 - val_loss: 0.5731\n",
            "Epoch 118/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5485 - val_loss: 0.5731\n",
            "Epoch 119/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5484 - val_loss: 0.5731\n",
            "Epoch 120/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5483 - val_loss: 0.5732\n",
            "Epoch 121/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5482 - val_loss: 0.5732\n",
            "Epoch 122/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5481 - val_loss: 0.5732\n",
            "Epoch 123/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5480 - val_loss: 0.5732\n",
            "Epoch 124/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5479 - val_loss: 0.5733\n",
            "Epoch 125/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5478 - val_loss: 0.5733\n",
            "Epoch 126/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5477 - val_loss: 0.5733\n",
            "Epoch 127/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5475 - val_loss: 0.5733\n",
            "Epoch 128/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5473 - val_loss: 0.5734\n",
            "Epoch 129/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5472 - val_loss: 0.5734\n",
            "Epoch 130/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5472 - val_loss: 0.5734\n",
            "Epoch 131/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5470 - val_loss: 0.5735\n",
            "Epoch 132/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5469 - val_loss: 0.5735\n",
            "Epoch 133/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5468 - val_loss: 0.5735\n",
            "Epoch 134/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5466 - val_loss: 0.5736\n",
            "Epoch 135/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5466 - val_loss: 0.5736\n",
            "Epoch 136/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5465 - val_loss: 0.5737\n",
            "Epoch 137/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5464 - val_loss: 0.5737\n",
            "Epoch 138/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5463 - val_loss: 0.5737\n",
            "Epoch 139/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5462 - val_loss: 0.5737\n",
            "Epoch 140/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5461 - val_loss: 0.5738\n",
            "Epoch 141/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5460 - val_loss: 0.5738\n",
            "Epoch 142/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5459 - val_loss: 0.5739\n",
            "Epoch 143/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5458 - val_loss: 0.5739\n",
            "Epoch 144/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5456 - val_loss: 0.5740\n",
            "Epoch 145/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5456 - val_loss: 0.5740\n",
            "Epoch 146/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5455 - val_loss: 0.5741\n",
            "Epoch 147/150\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5453 - val_loss: 0.5741\n",
            "Epoch 148/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5453 - val_loss: 0.5742\n",
            "Epoch 149/150\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5453 - val_loss: 0.5742\n",
            "Epoch 150/150\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5451 - val_loss: 0.5743\n",
            "test auc = 0.7924379881870169 in cv = 1\n",
            "\n",
            " cv 1 finished for assist2017\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 2 starts for assist2017\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/assist2017-cv-train-2.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/assist2017-cv-test-2.csv is loaded\n",
            "input dataset is raw, will perform encode and extract features\n",
            "problem encoder is created\n",
            "train and test data are encoded\n",
            "train and test sequences are extracted\n",
            "100% 1367/1367 [00:00<00:00, 52854.59it/s]\n",
            "100% 342/342 [00:00<00:00, 57705.85it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 4119, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/150\n",
            "5/5 [==============================] - 1s 50ms/step - loss: 0.7990 - val_loss: 0.7825\n",
            "Epoch 2/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7789 - val_loss: 0.7652\n",
            "Epoch 3/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.7619 - val_loss: 0.7499\n",
            "Epoch 4/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7466 - val_loss: 0.7363\n",
            "Epoch 5/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7328 - val_loss: 0.7240\n",
            "Epoch 6/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7204 - val_loss: 0.7129\n",
            "Epoch 7/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7089 - val_loss: 0.7028\n",
            "Epoch 8/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6985 - val_loss: 0.6936\n",
            "Epoch 9/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6889 - val_loss: 0.6850\n",
            "Epoch 10/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6802 - val_loss: 0.6773\n",
            "Epoch 11/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6723 - val_loss: 0.6704\n",
            "Epoch 12/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6650 - val_loss: 0.6640\n",
            "Epoch 13/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6582 - val_loss: 0.6580\n",
            "Epoch 14/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6520 - val_loss: 0.6526\n",
            "Epoch 15/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6462 - val_loss: 0.6477\n",
            "Epoch 16/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6410 - val_loss: 0.6431\n",
            "Epoch 17/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6362 - val_loss: 0.6389\n",
            "Epoch 18/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6318 - val_loss: 0.6352\n",
            "Epoch 19/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6279 - val_loss: 0.6318\n",
            "Epoch 20/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6242 - val_loss: 0.6285\n",
            "Epoch 21/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6207 - val_loss: 0.6254\n",
            "Epoch 22/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6174 - val_loss: 0.6225\n",
            "Epoch 23/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6144 - val_loss: 0.6199\n",
            "Epoch 24/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6117 - val_loss: 0.6175\n",
            "Epoch 25/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6091 - val_loss: 0.6153\n",
            "Epoch 26/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6066 - val_loss: 0.6132\n",
            "Epoch 27/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6043 - val_loss: 0.6113\n",
            "Epoch 28/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6021 - val_loss: 0.6095\n",
            "Epoch 29/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6000 - val_loss: 0.6079\n",
            "Epoch 30/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5983 - val_loss: 0.6062\n",
            "Epoch 31/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5965 - val_loss: 0.6046\n",
            "Epoch 32/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5947 - val_loss: 0.6032\n",
            "Epoch 33/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5931 - val_loss: 0.6020\n",
            "Epoch 34/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5916 - val_loss: 0.6008\n",
            "Epoch 35/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5903 - val_loss: 0.5997\n",
            "Epoch 36/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5890 - val_loss: 0.5985\n",
            "Epoch 37/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5877 - val_loss: 0.5974\n",
            "Epoch 38/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5866 - val_loss: 0.5964\n",
            "Epoch 39/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5855 - val_loss: 0.5955\n",
            "Epoch 40/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5844 - val_loss: 0.5947\n",
            "Epoch 41/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5834 - val_loss: 0.5940\n",
            "Epoch 42/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5825 - val_loss: 0.5932\n",
            "Epoch 43/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5814 - val_loss: 0.5926\n",
            "Epoch 44/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5805 - val_loss: 0.5919\n",
            "Epoch 45/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5797 - val_loss: 0.5913\n",
            "Epoch 46/150\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5790 - val_loss: 0.5907\n",
            "Epoch 47/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5782 - val_loss: 0.5901\n",
            "Epoch 48/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5774 - val_loss: 0.5896\n",
            "Epoch 49/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5768 - val_loss: 0.5890\n",
            "Epoch 50/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5762 - val_loss: 0.5886\n",
            "Epoch 51/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5755 - val_loss: 0.5882\n",
            "Epoch 52/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5749 - val_loss: 0.5877\n",
            "Epoch 53/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5743 - val_loss: 0.5873\n",
            "Epoch 54/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5736 - val_loss: 0.5869\n",
            "Epoch 55/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5730 - val_loss: 0.5866\n",
            "Epoch 56/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5724 - val_loss: 0.5862\n",
            "Epoch 57/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5719 - val_loss: 0.5858\n",
            "Epoch 58/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5714 - val_loss: 0.5855\n",
            "Epoch 59/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5709 - val_loss: 0.5852\n",
            "Epoch 60/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5704 - val_loss: 0.5849\n",
            "Epoch 61/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5699 - val_loss: 0.5845\n",
            "Epoch 62/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5694 - val_loss: 0.5842\n",
            "Epoch 63/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5689 - val_loss: 0.5839\n",
            "Epoch 64/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5685 - val_loss: 0.5837\n",
            "Epoch 65/150\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5682 - val_loss: 0.5837\n",
            "Epoch 66/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5678 - val_loss: 0.5836\n",
            "Epoch 67/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5675 - val_loss: 0.5835\n",
            "Epoch 68/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5671 - val_loss: 0.5832\n",
            "Epoch 69/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5668 - val_loss: 0.5830\n",
            "Epoch 70/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5664 - val_loss: 0.5828\n",
            "Epoch 71/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5661 - val_loss: 0.5827\n",
            "Epoch 72/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5658 - val_loss: 0.5826\n",
            "Epoch 73/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5655 - val_loss: 0.5824\n",
            "Epoch 74/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5652 - val_loss: 0.5822\n",
            "Epoch 75/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5649 - val_loss: 0.5820\n",
            "Epoch 76/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5646 - val_loss: 0.5818\n",
            "Epoch 77/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5643 - val_loss: 0.5816\n",
            "Epoch 78/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5640 - val_loss: 0.5814\n",
            "Epoch 79/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5637 - val_loss: 0.5812\n",
            "Epoch 80/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5635 - val_loss: 0.5810\n",
            "Epoch 81/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5632 - val_loss: 0.5809\n",
            "Epoch 82/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5630 - val_loss: 0.5808\n",
            "Epoch 83/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5628 - val_loss: 0.5807\n",
            "Epoch 84/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5625 - val_loss: 0.5807\n",
            "Epoch 85/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5623 - val_loss: 0.5807\n",
            "Epoch 86/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5621 - val_loss: 0.5806\n",
            "Epoch 87/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5619 - val_loss: 0.5806\n",
            "Epoch 88/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5616 - val_loss: 0.5804\n",
            "Epoch 89/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5613 - val_loss: 0.5803\n",
            "Epoch 90/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5612 - val_loss: 0.5802\n",
            "Epoch 91/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5610 - val_loss: 0.5800\n",
            "Epoch 92/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5608 - val_loss: 0.5800\n",
            "Epoch 93/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5606 - val_loss: 0.5799\n",
            "Epoch 94/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5603 - val_loss: 0.5799\n",
            "Epoch 95/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5601 - val_loss: 0.5799\n",
            "Epoch 96/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5599 - val_loss: 0.5799\n",
            "Epoch 97/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5597 - val_loss: 0.5799\n",
            "Epoch 98/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5596 - val_loss: 0.5800\n",
            "Epoch 99/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5595 - val_loss: 0.5800\n",
            "Epoch 100/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5593 - val_loss: 0.5799\n",
            "Epoch 101/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5591 - val_loss: 0.5797\n",
            "Epoch 102/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5588 - val_loss: 0.5796\n",
            "Epoch 103/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5587 - val_loss: 0.5796\n",
            "Epoch 104/150\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5585 - val_loss: 0.5795\n",
            "Epoch 105/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5584 - val_loss: 0.5794\n",
            "Epoch 106/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5581 - val_loss: 0.5794\n",
            "Epoch 107/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5581 - val_loss: 0.5794\n",
            "Epoch 108/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5579 - val_loss: 0.5794\n",
            "Epoch 109/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5579 - val_loss: 0.5794\n",
            "Epoch 110/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5577 - val_loss: 0.5795\n",
            "Epoch 111/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5576 - val_loss: 0.5795\n",
            "Epoch 112/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5574 - val_loss: 0.5794\n",
            "Epoch 113/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5573 - val_loss: 0.5792\n",
            "Epoch 114/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5571 - val_loss: 0.5791\n",
            "Epoch 115/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5570 - val_loss: 0.5790\n",
            "Epoch 116/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5569 - val_loss: 0.5790\n",
            "Epoch 117/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5567 - val_loss: 0.5789\n",
            "Epoch 118/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5566 - val_loss: 0.5789\n",
            "Epoch 119/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5563 - val_loss: 0.5789\n",
            "Epoch 120/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5563 - val_loss: 0.5789\n",
            "Epoch 121/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5562 - val_loss: 0.5789\n",
            "Epoch 122/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5560 - val_loss: 0.5788\n",
            "Epoch 123/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5559 - val_loss: 0.5789\n",
            "Epoch 124/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5557 - val_loss: 0.5788\n",
            "Epoch 125/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5557 - val_loss: 0.5789\n",
            "Epoch 126/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5555 - val_loss: 0.5789\n",
            "Epoch 127/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5554 - val_loss: 0.5789\n",
            "Epoch 128/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5553 - val_loss: 0.5789\n",
            "Epoch 129/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5553 - val_loss: 0.5788\n",
            "Epoch 130/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5551 - val_loss: 0.5788\n",
            "Epoch 131/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5550 - val_loss: 0.5787\n",
            "Epoch 132/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5549 - val_loss: 0.5787\n",
            "Epoch 133/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5547 - val_loss: 0.5787\n",
            "Epoch 134/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5546 - val_loss: 0.5787\n",
            "Epoch 135/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5545 - val_loss: 0.5788\n",
            "Epoch 136/150\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5544 - val_loss: 0.5787\n",
            "Epoch 137/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5544 - val_loss: 0.5787\n",
            "Epoch 138/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5542 - val_loss: 0.5787\n",
            "Epoch 139/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5542 - val_loss: 0.5787\n",
            "Epoch 140/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5541 - val_loss: 0.5788\n",
            "Epoch 141/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5542 - val_loss: 0.5788\n",
            "Epoch 142/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5541 - val_loss: 0.5788\n",
            "Epoch 143/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5539 - val_loss: 0.5788\n",
            "Epoch 144/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5539 - val_loss: 0.5788\n",
            "Epoch 145/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5537 - val_loss: 0.5787\n",
            "Epoch 146/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5537 - val_loss: 0.5787\n",
            "Epoch 147/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5536 - val_loss: 0.5788\n",
            "Epoch 148/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5536 - val_loss: 0.5789\n",
            "Epoch 149/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5534 - val_loss: 0.5790\n",
            "Epoch 150/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5535 - val_loss: 0.5790\n",
            "test auc = 0.7911807452819004 in cv = 2\n",
            "\n",
            " cv 2 finished for assist2017\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 3 starts for assist2017\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/assist2017-cv-train-3.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/assist2017-cv-test-3.csv is loaded\n",
            "input dataset is raw, will perform encode and extract features\n",
            "problem encoder is created\n",
            "train and test data are encoded\n",
            "train and test sequences are extracted\n",
            "100% 1367/1367 [00:00<00:00, 51597.00it/s]\n",
            "100% 342/342 [00:00<00:00, 61551.25it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 4119, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/150\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.8047 - val_loss: 0.7909\n",
            "Epoch 2/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.7838 - val_loss: 0.7727\n",
            "Epoch 3/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.7659 - val_loss: 0.7564\n",
            "Epoch 4/150\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.7495 - val_loss: 0.7412\n",
            "Epoch 5/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7342 - val_loss: 0.7269\n",
            "Epoch 6/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7200 - val_loss: 0.7133\n",
            "Epoch 7/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.7066 - val_loss: 0.7008\n",
            "Epoch 8/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6943 - val_loss: 0.6895\n",
            "Epoch 9/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6832 - val_loss: 0.6794\n",
            "Epoch 10/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6731 - val_loss: 0.6704\n",
            "Epoch 11/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6640 - val_loss: 0.6624\n",
            "Epoch 12/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6558 - val_loss: 0.6553\n",
            "Epoch 13/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6485 - val_loss: 0.6490\n",
            "Epoch 14/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6419 - val_loss: 0.6433\n",
            "Epoch 15/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6360 - val_loss: 0.6382\n",
            "Epoch 16/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6306 - val_loss: 0.6337\n",
            "Epoch 17/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6257 - val_loss: 0.6296\n",
            "Epoch 18/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6213 - val_loss: 0.6259\n",
            "Epoch 19/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6173 - val_loss: 0.6224\n",
            "Epoch 20/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6137 - val_loss: 0.6192\n",
            "Epoch 21/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6103 - val_loss: 0.6162\n",
            "Epoch 22/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6072 - val_loss: 0.6134\n",
            "Epoch 23/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6043 - val_loss: 0.6109\n",
            "Epoch 24/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6017 - val_loss: 0.6087\n",
            "Epoch 25/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5992 - val_loss: 0.6065\n",
            "Epoch 26/150\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5969 - val_loss: 0.6045\n",
            "Epoch 27/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5947 - val_loss: 0.6026\n",
            "Epoch 28/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5928 - val_loss: 0.6010\n",
            "Epoch 29/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5910 - val_loss: 0.5995\n",
            "Epoch 30/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5893 - val_loss: 0.5981\n",
            "Epoch 31/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5877 - val_loss: 0.5968\n",
            "Epoch 32/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5862 - val_loss: 0.5955\n",
            "Epoch 33/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5848 - val_loss: 0.5945\n",
            "Epoch 34/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5834 - val_loss: 0.5935\n",
            "Epoch 35/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5823 - val_loss: 0.5926\n",
            "Epoch 36/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5811 - val_loss: 0.5917\n",
            "Epoch 37/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5800 - val_loss: 0.5910\n",
            "Epoch 38/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5790 - val_loss: 0.5902\n",
            "Epoch 39/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5780 - val_loss: 0.5895\n",
            "Epoch 40/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5770 - val_loss: 0.5887\n",
            "Epoch 41/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5762 - val_loss: 0.5880\n",
            "Epoch 42/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5753 - val_loss: 0.5874\n",
            "Epoch 43/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5745 - val_loss: 0.5868\n",
            "Epoch 44/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5737 - val_loss: 0.5863\n",
            "Epoch 45/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5730 - val_loss: 0.5858\n",
            "Epoch 46/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5723 - val_loss: 0.5854\n",
            "Epoch 47/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5716 - val_loss: 0.5850\n",
            "Epoch 48/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5710 - val_loss: 0.5846\n",
            "Epoch 49/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5703 - val_loss: 0.5842\n",
            "Epoch 50/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5698 - val_loss: 0.5838\n",
            "Epoch 51/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5692 - val_loss: 0.5833\n",
            "Epoch 52/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5687 - val_loss: 0.5828\n",
            "Epoch 53/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5682 - val_loss: 0.5825\n",
            "Epoch 54/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5676 - val_loss: 0.5821\n",
            "Epoch 55/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5672 - val_loss: 0.5818\n",
            "Epoch 56/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5666 - val_loss: 0.5816\n",
            "Epoch 57/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5661 - val_loss: 0.5813\n",
            "Epoch 58/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5656 - val_loss: 0.5809\n",
            "Epoch 59/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5652 - val_loss: 0.5805\n",
            "Epoch 60/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5649 - val_loss: 0.5803\n",
            "Epoch 61/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5645 - val_loss: 0.5802\n",
            "Epoch 62/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5641 - val_loss: 0.5800\n",
            "Epoch 63/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5637 - val_loss: 0.5799\n",
            "Epoch 64/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5634 - val_loss: 0.5797\n",
            "Epoch 65/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5630 - val_loss: 0.5795\n",
            "Epoch 66/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5626 - val_loss: 0.5793\n",
            "Epoch 67/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5622 - val_loss: 0.5791\n",
            "Epoch 68/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5619 - val_loss: 0.5789\n",
            "Epoch 69/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5616 - val_loss: 0.5786\n",
            "Epoch 70/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5613 - val_loss: 0.5784\n",
            "Epoch 71/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5609 - val_loss: 0.5782\n",
            "Epoch 72/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5606 - val_loss: 0.5781\n",
            "Epoch 73/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5604 - val_loss: 0.5780\n",
            "Epoch 74/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5602 - val_loss: 0.5779\n",
            "Epoch 75/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5598 - val_loss: 0.5777\n",
            "Epoch 76/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5596 - val_loss: 0.5777\n",
            "Epoch 77/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5593 - val_loss: 0.5775\n",
            "Epoch 78/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5590 - val_loss: 0.5774\n",
            "Epoch 79/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5587 - val_loss: 0.5773\n",
            "Epoch 80/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5584 - val_loss: 0.5772\n",
            "Epoch 81/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5582 - val_loss: 0.5770\n",
            "Epoch 82/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5579 - val_loss: 0.5769\n",
            "Epoch 83/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5577 - val_loss: 0.5768\n",
            "Epoch 84/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5574 - val_loss: 0.5767\n",
            "Epoch 85/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5573 - val_loss: 0.5766\n",
            "Epoch 86/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5571 - val_loss: 0.5765\n",
            "Epoch 87/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5569 - val_loss: 0.5764\n",
            "Epoch 88/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5567 - val_loss: 0.5763\n",
            "Epoch 89/150\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5564 - val_loss: 0.5762\n",
            "Epoch 90/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5562 - val_loss: 0.5761\n",
            "Epoch 91/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5560 - val_loss: 0.5760\n",
            "Epoch 92/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5559 - val_loss: 0.5758\n",
            "Epoch 93/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5557 - val_loss: 0.5758\n",
            "Epoch 94/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5555 - val_loss: 0.5757\n",
            "Epoch 95/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5555 - val_loss: 0.5757\n",
            "Epoch 96/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5552 - val_loss: 0.5758\n",
            "Epoch 97/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5551 - val_loss: 0.5758\n",
            "Epoch 98/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5549 - val_loss: 0.5757\n",
            "Epoch 99/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5547 - val_loss: 0.5757\n",
            "Epoch 100/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5546 - val_loss: 0.5757\n",
            "Epoch 101/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5543 - val_loss: 0.5757\n",
            "Epoch 102/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5543 - val_loss: 0.5757\n",
            "Epoch 103/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5541 - val_loss: 0.5756\n",
            "Epoch 104/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5539 - val_loss: 0.5756\n",
            "Epoch 105/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5538 - val_loss: 0.5756\n",
            "Epoch 106/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5537 - val_loss: 0.5756\n",
            "Epoch 107/150\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5535 - val_loss: 0.5756\n",
            "Epoch 108/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5534 - val_loss: 0.5757\n",
            "Epoch 109/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5533 - val_loss: 0.5756\n",
            "Epoch 110/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5531 - val_loss: 0.5755\n",
            "Epoch 111/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5529 - val_loss: 0.5755\n",
            "Epoch 112/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5529 - val_loss: 0.5755\n",
            "Epoch 113/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5527 - val_loss: 0.5754\n",
            "Epoch 114/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5527 - val_loss: 0.5753\n",
            "Epoch 115/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5524 - val_loss: 0.5754\n",
            "Epoch 116/150\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5523 - val_loss: 0.5754\n",
            "Epoch 117/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5522 - val_loss: 0.5754\n",
            "Epoch 118/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5521 - val_loss: 0.5754\n",
            "Epoch 119/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5520 - val_loss: 0.5753\n",
            "Epoch 120/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5520 - val_loss: 0.5753\n",
            "Epoch 121/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5519 - val_loss: 0.5753\n",
            "Epoch 122/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5517 - val_loss: 0.5752\n",
            "Epoch 123/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5516 - val_loss: 0.5751\n",
            "Epoch 124/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5515 - val_loss: 0.5751\n",
            "Epoch 125/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5514 - val_loss: 0.5751\n",
            "Epoch 126/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5512 - val_loss: 0.5751\n",
            "Epoch 127/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5511 - val_loss: 0.5750\n",
            "Epoch 128/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5511 - val_loss: 0.5750\n",
            "Epoch 129/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5510 - val_loss: 0.5750\n",
            "Epoch 130/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5508 - val_loss: 0.5751\n",
            "Epoch 131/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5507 - val_loss: 0.5751\n",
            "Epoch 132/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5506 - val_loss: 0.5751\n",
            "Epoch 133/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5505 - val_loss: 0.5751\n",
            "Epoch 134/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5504 - val_loss: 0.5752\n",
            "Epoch 135/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5503 - val_loss: 0.5752\n",
            "Epoch 136/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5502 - val_loss: 0.5752\n",
            "Epoch 137/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5502 - val_loss: 0.5751\n",
            "Epoch 138/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5500 - val_loss: 0.5751\n",
            "Epoch 139/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5499 - val_loss: 0.5751\n",
            "Epoch 140/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5498 - val_loss: 0.5750\n",
            "Epoch 141/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5498 - val_loss: 0.5750\n",
            "Epoch 142/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5496 - val_loss: 0.5749\n",
            "Epoch 143/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5496 - val_loss: 0.5749\n",
            "Epoch 144/150\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5496 - val_loss: 0.5749\n",
            "Epoch 145/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5495 - val_loss: 0.5749\n",
            "Epoch 146/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5494 - val_loss: 0.5749\n",
            "Epoch 147/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5494 - val_loss: 0.5749\n",
            "Epoch 148/150\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5493 - val_loss: 0.5748\n",
            "Epoch 149/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5491 - val_loss: 0.5748\n",
            "Epoch 150/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5492 - val_loss: 0.5748\n",
            "test auc = 0.7851133672714903 in cv = 3\n",
            "\n",
            " cv 3 finished for assist2017\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 4 starts for assist2017\n",
            "\n",
            "train data in /content/dirve/My Drive/dpfa/data/assist2017-cv-train-4.csv is loaded\n",
            "test data in /content/dirve/My Drive/dpfa/data/assist2017-cv-test-4.csv is loaded\n",
            "input dataset is raw, will perform encode and extract features\n",
            "problem encoder is created\n",
            "train and test data are encoded\n",
            "train and test sequences are extracted\n",
            "100% 1368/1368 [00:00<00:00, 53266.44it/s]\n",
            "100% 341/341 [00:00<00:00, 55531.05it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 16, 'item_vocab_size': 4119, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': False}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/150\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.8102 - val_loss: 0.7982\n",
            "Epoch 2/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.7904 - val_loss: 0.7813\n",
            "Epoch 3/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7739 - val_loss: 0.7663\n",
            "Epoch 4/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7590 - val_loss: 0.7524\n",
            "Epoch 5/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7452 - val_loss: 0.7397\n",
            "Epoch 6/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7325 - val_loss: 0.7279\n",
            "Epoch 7/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.7209 - val_loss: 0.7171\n",
            "Epoch 8/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7100 - val_loss: 0.7071\n",
            "Epoch 9/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7000 - val_loss: 0.6977\n",
            "Epoch 10/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6906 - val_loss: 0.6890\n",
            "Epoch 11/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6818 - val_loss: 0.6809\n",
            "Epoch 12/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6738 - val_loss: 0.6736\n",
            "Epoch 13/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6665 - val_loss: 0.6670\n",
            "Epoch 14/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6596 - val_loss: 0.6609\n",
            "Epoch 15/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6532 - val_loss: 0.6554\n",
            "Epoch 16/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6475 - val_loss: 0.6504\n",
            "Epoch 17/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6423 - val_loss: 0.6459\n",
            "Epoch 18/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6376 - val_loss: 0.6418\n",
            "Epoch 19/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6331 - val_loss: 0.6379\n",
            "Epoch 20/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6289 - val_loss: 0.6342\n",
            "Epoch 21/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6252 - val_loss: 0.6310\n",
            "Epoch 22/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6218 - val_loss: 0.6280\n",
            "Epoch 23/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6185 - val_loss: 0.6252\n",
            "Epoch 24/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6155 - val_loss: 0.6225\n",
            "Epoch 25/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6127 - val_loss: 0.6198\n",
            "Epoch 26/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6101 - val_loss: 0.6174\n",
            "Epoch 27/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6076 - val_loss: 0.6151\n",
            "Epoch 28/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6053 - val_loss: 0.6129\n",
            "Epoch 29/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6031 - val_loss: 0.6110\n",
            "Epoch 30/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6012 - val_loss: 0.6092\n",
            "Epoch 31/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5993 - val_loss: 0.6075\n",
            "Epoch 32/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5975 - val_loss: 0.6059\n",
            "Epoch 33/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5958 - val_loss: 0.6044\n",
            "Epoch 34/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5942 - val_loss: 0.6030\n",
            "Epoch 35/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5927 - val_loss: 0.6018\n",
            "Epoch 36/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5913 - val_loss: 0.6005\n",
            "Epoch 37/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5899 - val_loss: 0.5992\n",
            "Epoch 38/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5886 - val_loss: 0.5981\n",
            "Epoch 39/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5874 - val_loss: 0.5971\n",
            "Epoch 40/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5863 - val_loss: 0.5960\n",
            "Epoch 41/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5852 - val_loss: 0.5950\n",
            "Epoch 42/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5841 - val_loss: 0.5941\n",
            "Epoch 43/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5830 - val_loss: 0.5933\n",
            "Epoch 44/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5821 - val_loss: 0.5926\n",
            "Epoch 45/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5811 - val_loss: 0.5920\n",
            "Epoch 46/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5803 - val_loss: 0.5913\n",
            "Epoch 47/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5795 - val_loss: 0.5907\n",
            "Epoch 48/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5787 - val_loss: 0.5901\n",
            "Epoch 49/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5779 - val_loss: 0.5895\n",
            "Epoch 50/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5772 - val_loss: 0.5890\n",
            "Epoch 51/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5765 - val_loss: 0.5885\n",
            "Epoch 52/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5760 - val_loss: 0.5880\n",
            "Epoch 53/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5753 - val_loss: 0.5876\n",
            "Epoch 54/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5747 - val_loss: 0.5871\n",
            "Epoch 55/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5742 - val_loss: 0.5867\n",
            "Epoch 56/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5736 - val_loss: 0.5863\n",
            "Epoch 57/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5731 - val_loss: 0.5858\n",
            "Epoch 58/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5725 - val_loss: 0.5854\n",
            "Epoch 59/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5719 - val_loss: 0.5850\n",
            "Epoch 60/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5715 - val_loss: 0.5847\n",
            "Epoch 61/150\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5710 - val_loss: 0.5844\n",
            "Epoch 62/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5705 - val_loss: 0.5842\n",
            "Epoch 63/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5701 - val_loss: 0.5840\n",
            "Epoch 64/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5697 - val_loss: 0.5839\n",
            "Epoch 65/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5694 - val_loss: 0.5836\n",
            "Epoch 66/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5690 - val_loss: 0.5834\n",
            "Epoch 67/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5686 - val_loss: 0.5831\n",
            "Epoch 68/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5682 - val_loss: 0.5828\n",
            "Epoch 69/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5678 - val_loss: 0.5826\n",
            "Epoch 70/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5676 - val_loss: 0.5823\n",
            "Epoch 71/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5672 - val_loss: 0.5821\n",
            "Epoch 72/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5670 - val_loss: 0.5819\n",
            "Epoch 73/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5665 - val_loss: 0.5817\n",
            "Epoch 74/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5663 - val_loss: 0.5814\n",
            "Epoch 75/150\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.5659 - val_loss: 0.5812\n",
            "Epoch 76/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5656 - val_loss: 0.5810\n",
            "Epoch 77/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5653 - val_loss: 0.5807\n",
            "Epoch 78/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5650 - val_loss: 0.5805\n",
            "Epoch 79/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5647 - val_loss: 0.5804\n",
            "Epoch 80/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5644 - val_loss: 0.5803\n",
            "Epoch 81/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5641 - val_loss: 0.5803\n",
            "Epoch 82/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5639 - val_loss: 0.5802\n",
            "Epoch 83/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5637 - val_loss: 0.5802\n",
            "Epoch 84/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5634 - val_loss: 0.5801\n",
            "Epoch 85/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5631 - val_loss: 0.5800\n",
            "Epoch 86/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5629 - val_loss: 0.5799\n",
            "Epoch 87/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5627 - val_loss: 0.5799\n",
            "Epoch 88/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5624 - val_loss: 0.5799\n",
            "Epoch 89/150\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5622 - val_loss: 0.5798\n",
            "Epoch 90/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5620 - val_loss: 0.5797\n",
            "Epoch 91/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5618 - val_loss: 0.5796\n",
            "Epoch 92/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5616 - val_loss: 0.5795\n",
            "Epoch 93/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5613 - val_loss: 0.5794\n",
            "Epoch 94/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5612 - val_loss: 0.5793\n",
            "Epoch 95/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5610 - val_loss: 0.5793\n",
            "Epoch 96/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5607 - val_loss: 0.5791\n",
            "Epoch 97/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5605 - val_loss: 0.5791\n",
            "Epoch 98/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5603 - val_loss: 0.5790\n",
            "Epoch 99/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5601 - val_loss: 0.5790\n",
            "Epoch 100/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5599 - val_loss: 0.5790\n",
            "Epoch 101/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5598 - val_loss: 0.5790\n",
            "Epoch 102/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5596 - val_loss: 0.5789\n",
            "Epoch 103/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5594 - val_loss: 0.5788\n",
            "Epoch 104/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5592 - val_loss: 0.5787\n",
            "Epoch 105/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5590 - val_loss: 0.5786\n",
            "Epoch 106/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5588 - val_loss: 0.5785\n",
            "Epoch 107/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5587 - val_loss: 0.5784\n",
            "Epoch 108/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5585 - val_loss: 0.5783\n",
            "Epoch 109/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5584 - val_loss: 0.5782\n",
            "Epoch 110/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5582 - val_loss: 0.5781\n",
            "Epoch 111/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5582 - val_loss: 0.5780\n",
            "Epoch 112/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5580 - val_loss: 0.5779\n",
            "Epoch 113/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5579 - val_loss: 0.5778\n",
            "Epoch 114/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5577 - val_loss: 0.5778\n",
            "Epoch 115/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5577 - val_loss: 0.5778\n",
            "Epoch 116/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5574 - val_loss: 0.5778\n",
            "Epoch 117/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5573 - val_loss: 0.5777\n",
            "Epoch 118/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5572 - val_loss: 0.5776\n",
            "Epoch 119/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5570 - val_loss: 0.5775\n",
            "Epoch 120/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5569 - val_loss: 0.5775\n",
            "Epoch 121/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5568 - val_loss: 0.5774\n",
            "Epoch 122/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5566 - val_loss: 0.5774\n",
            "Epoch 123/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5566 - val_loss: 0.5774\n",
            "Epoch 124/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5565 - val_loss: 0.5774\n",
            "Epoch 125/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5564 - val_loss: 0.5773\n",
            "Epoch 126/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5563 - val_loss: 0.5771\n",
            "Epoch 127/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5561 - val_loss: 0.5770\n",
            "Epoch 128/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5560 - val_loss: 0.5770\n",
            "Epoch 129/150\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.5559 - val_loss: 0.5770\n",
            "Epoch 130/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5558 - val_loss: 0.5770\n",
            "Epoch 131/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5557 - val_loss: 0.5770\n",
            "Epoch 132/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5556 - val_loss: 0.5769\n",
            "Epoch 133/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5555 - val_loss: 0.5768\n",
            "Epoch 134/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5554 - val_loss: 0.5768\n",
            "Epoch 135/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5553 - val_loss: 0.5767\n",
            "Epoch 136/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5552 - val_loss: 0.5767\n",
            "Epoch 137/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5551 - val_loss: 0.5766\n",
            "Epoch 138/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5549 - val_loss: 0.5766\n",
            "Epoch 139/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5547 - val_loss: 0.5767\n",
            "Epoch 140/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5547 - val_loss: 0.5767\n",
            "Epoch 141/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5546 - val_loss: 0.5767\n",
            "Epoch 142/150\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.5545 - val_loss: 0.5767\n",
            "Epoch 143/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5545 - val_loss: 0.5767\n",
            "Epoch 144/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5543 - val_loss: 0.5767\n",
            "Epoch 145/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5542 - val_loss: 0.5766\n",
            "Epoch 146/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5541 - val_loss: 0.5766\n",
            "Epoch 147/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5540 - val_loss: 0.5767\n",
            "Epoch 148/150\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.5539 - val_loss: 0.5767\n",
            "Epoch 149/150\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5538 - val_loss: 0.5767\n",
            "Epoch 150/150\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5537 - val_loss: 0.5767\n",
            "test auc = 0.7888475292773038 in cv = 4\n",
            "\n",
            " cv 4 finished for assist2017\n",
            "\n",
            "average test auc for assist2017 is 0.7895495002863429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVJRVHVRVvj5",
        "outputId": "736efd24-9c67-4089-e607-7e6968d7a74b"
      },
      "source": [
        "!python main.py --dataset 'nips2020' --dir '/content/dirve/My Drive/ktrace/data'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-27 00:34:40.461475: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "fit params are {'batch_size': 500, 'epochs': 20, 'validation_split': 0.1}\n",
            "optimizer and loss params = {'lr': 0.006, 'smoothing': 0.1}\n",
            "session cleared\n",
            "\n",
            " cv 0 starts for nips2020\n",
            "\n",
            "train data in /content/dirve/My Drive/ktrace/data/nips2020-cv-train-0.csv is loaded\n",
            "test data in /content/dirve/My Drive/ktrace/data/nips2020-cv-test-0.csv is loaded\n",
            "dataset = nips2020\n",
            "num of attempts = 15867.85 K\n",
            "num of students = 118971\n",
            "num of items = 27616\n",
            "input dataset is already a feature datasetskipping the encoding and feature extraction step\n",
            "100% 95176/95176 [00:04<00:00, 23455.91it/s]\n",
            "100% 23795/23795 [00:00<00:00, 66203.65it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "2021-07-27 00:35:40.740057: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-27 00:35:40.748647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:35:40.749365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-07-27 00:35:40.749416: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-27 00:35:40.751979: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-07-27 00:35:40.752079: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-07-27 00:35:40.754031: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-27 00:35:40.754433: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-27 00:35:40.756180: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-27 00:35:40.756731: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-07-27 00:35:40.756946: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-07-27 00:35:40.757081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:35:40.757814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:35:40.758453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-07-27 00:35:40.758861: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-07-27 00:35:40.759184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:35:40.759863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-07-27 00:35:40.759970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:35:40.760636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:35:40.761276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-07-27 00:35:40.761336: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-27 00:35:41.197818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-27 00:35:41.197879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-07-27 00:35:41.197893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-07-27 00:35:41.198124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:35:41.198900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:35:41.199596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-27 00:35:41.200242: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-07-27 00:35:41.200297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14682 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "WARNING:tensorflow:From /content/dirve/My Drive/dpfa/dpfa/model.py:40: DenseEinsum.__init__ (from official.nlp.modeling.layers.dense_einsum) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "DenseEinsum is deprecated. Please use tf.keras.experimental.EinsumDense layer instead.\n",
            "model params are {'hidden_size': 32, 'item_vocab_size': 27616, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': True}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "2021-07-27 00:35:41.265131: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-07-27 00:35:41.265585: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000165000 Hz\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "2021-07-27 00:35:42.875487: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-07-27 00:35:43.290196: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "213/213 [==============================] - 8s 29ms/step - loss: 0.7192 - val_loss: 0.6496\n",
            "Epoch 2/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.6197 - val_loss: 0.6000\n",
            "Epoch 3/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5879 - val_loss: 0.5813\n",
            "Epoch 4/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5741 - val_loss: 0.5724\n",
            "Epoch 5/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5668 - val_loss: 0.5673\n",
            "Epoch 6/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5623 - val_loss: 0.5643\n",
            "Epoch 7/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5593 - val_loss: 0.5623\n",
            "Epoch 8/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5573 - val_loss: 0.5610\n",
            "Epoch 9/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5558 - val_loss: 0.5600\n",
            "Epoch 10/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5547 - val_loss: 0.5596\n",
            "Epoch 11/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5538 - val_loss: 0.5589\n",
            "Epoch 12/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5531 - val_loss: 0.5584\n",
            "Epoch 13/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5526 - val_loss: 0.5582\n",
            "Epoch 14/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5521 - val_loss: 0.5580\n",
            "Epoch 15/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5517 - val_loss: 0.5579\n",
            "Epoch 16/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5514 - val_loss: 0.5577\n",
            "Epoch 17/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5511 - val_loss: 0.5575\n",
            "Epoch 18/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5508 - val_loss: 0.5574\n",
            "Epoch 19/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5506 - val_loss: 0.5575\n",
            "Epoch 20/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5504 - val_loss: 0.5573\n",
            "test auc = 0.7997622738689396 in cv = 0\n",
            "\n",
            " cv 0 finished for nips2020\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 1 starts for nips2020\n",
            "\n",
            "train data in /content/dirve/My Drive/ktrace/data/nips2020-cv-train-1.csv is loaded\n",
            "test data in /content/dirve/My Drive/ktrace/data/nips2020-cv-test-1.csv is loaded\n",
            "input dataset is already a feature datasetskipping the encoding and feature extraction step\n",
            "100% 95177/95177 [00:03<00:00, 29518.89it/s]\n",
            "100% 23794/23794 [00:00<00:00, 27071.56it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 32, 'item_vocab_size': 27616, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': True}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/20\n",
            "213/213 [==============================] - 7s 28ms/step - loss: 0.7407 - val_loss: 0.6636\n",
            "Epoch 2/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.6335 - val_loss: 0.6113\n",
            "Epoch 3/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5993 - val_loss: 0.5901\n",
            "Epoch 4/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5832 - val_loss: 0.5789\n",
            "Epoch 5/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5739 - val_loss: 0.5723\n",
            "Epoch 6/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5681 - val_loss: 0.5681\n",
            "Epoch 7/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5640 - val_loss: 0.5652\n",
            "Epoch 8/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5612 - val_loss: 0.5632\n",
            "Epoch 9/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5590 - val_loss: 0.5618\n",
            "Epoch 10/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5574 - val_loss: 0.5607\n",
            "Epoch 11/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5561 - val_loss: 0.5598\n",
            "Epoch 12/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5550 - val_loss: 0.5592\n",
            "Epoch 13/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5542 - val_loss: 0.5587\n",
            "Epoch 14/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5535 - val_loss: 0.5583\n",
            "Epoch 15/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5530 - val_loss: 0.5580\n",
            "Epoch 16/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5524 - val_loss: 0.5577\n",
            "Epoch 17/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5520 - val_loss: 0.5575\n",
            "Epoch 18/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5516 - val_loss: 0.5574\n",
            "Epoch 19/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5513 - val_loss: 0.5571\n",
            "Epoch 20/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5510 - val_loss: 0.5570\n",
            "test auc = 0.799004245519008 in cv = 1\n",
            "\n",
            " cv 1 finished for nips2020\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 2 starts for nips2020\n",
            "\n",
            "train data in /content/dirve/My Drive/ktrace/data/nips2020-cv-train-2.csv is loaded\n",
            "test data in /content/dirve/My Drive/ktrace/data/nips2020-cv-test-2.csv is loaded\n",
            "input dataset is already a feature datasetskipping the encoding and feature extraction step\n",
            "100% 95177/95177 [00:03<00:00, 28314.06it/s]\n",
            "100% 23794/23794 [00:00<00:00, 65615.84it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 32, 'item_vocab_size': 27616, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': True}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/20\n",
            "213/213 [==============================] - 7s 28ms/step - loss: 0.7669 - val_loss: 0.6780\n",
            "Epoch 2/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.6418 - val_loss: 0.6185\n",
            "Epoch 3/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.6050 - val_loss: 0.5953\n",
            "Epoch 4/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5877 - val_loss: 0.5830\n",
            "Epoch 5/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5777 - val_loss: 0.5756\n",
            "Epoch 6/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5712 - val_loss: 0.5707\n",
            "Epoch 7/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5667 - val_loss: 0.5674\n",
            "Epoch 8/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5634 - val_loss: 0.5650\n",
            "Epoch 9/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5610 - val_loss: 0.5632\n",
            "Epoch 10/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5591 - val_loss: 0.5620\n",
            "Epoch 11/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5576 - val_loss: 0.5609\n",
            "Epoch 12/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5564 - val_loss: 0.5602\n",
            "Epoch 13/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5553 - val_loss: 0.5595\n",
            "Epoch 14/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5545 - val_loss: 0.5590\n",
            "Epoch 15/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5538 - val_loss: 0.5586\n",
            "Epoch 16/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5532 - val_loss: 0.5582\n",
            "Epoch 17/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5527 - val_loss: 0.5579\n",
            "Epoch 18/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5523 - val_loss: 0.5576\n",
            "Epoch 19/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5519 - val_loss: 0.5574\n",
            "Epoch 20/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5515 - val_loss: 0.5573\n",
            "test auc = 0.7985665133343213 in cv = 2\n",
            "\n",
            " cv 2 finished for nips2020\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 3 starts for nips2020\n",
            "\n",
            "train data in /content/dirve/My Drive/ktrace/data/nips2020-cv-train-3.csv is loaded\n",
            "test data in /content/dirve/My Drive/ktrace/data/nips2020-cv-test-3.csv is loaded\n",
            "input dataset is already a feature datasetskipping the encoding and feature extraction step\n",
            "100% 95177/95177 [00:03<00:00, 27551.05it/s]\n",
            "100% 23794/23794 [00:00<00:00, 62106.09it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 32, 'item_vocab_size': 27616, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': True}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/20\n",
            "213/213 [==============================] - 7s 29ms/step - loss: 0.7205 - val_loss: 0.6451\n",
            "Epoch 2/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.6160 - val_loss: 0.5971\n",
            "Epoch 3/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5851 - val_loss: 0.5789\n",
            "Epoch 4/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5717 - val_loss: 0.5702\n",
            "Epoch 5/20\n",
            "213/213 [==============================] - 7s 35ms/step - loss: 0.5647 - val_loss: 0.5655\n",
            "Epoch 6/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5606 - val_loss: 0.5627\n",
            "Epoch 7/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5579 - val_loss: 0.5609\n",
            "Epoch 8/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5560 - val_loss: 0.5597\n",
            "Epoch 9/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5547 - val_loss: 0.5589\n",
            "Epoch 10/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5537 - val_loss: 0.5582\n",
            "Epoch 11/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5529 - val_loss: 0.5579\n",
            "Epoch 12/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5523 - val_loss: 0.5575\n",
            "Epoch 13/20\n",
            "213/213 [==============================] - 7s 35ms/step - loss: 0.5518 - val_loss: 0.5574\n",
            "Epoch 14/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5514 - val_loss: 0.5571\n",
            "Epoch 15/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5510 - val_loss: 0.5570\n",
            "Epoch 16/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5507 - val_loss: 0.5568\n",
            "Epoch 17/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5504 - val_loss: 0.5568\n",
            "Epoch 18/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5502 - val_loss: 0.5568\n",
            "Epoch 19/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5500 - val_loss: 0.5568\n",
            "Epoch 20/20\n",
            "213/213 [==============================] - 7s 34ms/step - loss: 0.5498 - val_loss: 0.5566\n",
            "test auc = 0.7978562727369725 in cv = 3\n",
            "\n",
            " cv 3 finished for nips2020\n",
            "\n",
            "session cleared\n",
            "\n",
            " cv 4 starts for nips2020\n",
            "\n",
            "train data in /content/dirve/My Drive/ktrace/data/nips2020-cv-train-4.csv is loaded\n",
            "test data in /content/dirve/My Drive/ktrace/data/nips2020-cv-test-4.csv is loaded\n",
            "input dataset is already a feature datasetskipping the encoding and feature extraction step\n",
            "100% 95177/95177 [00:03<00:00, 27241.84it/s]\n",
            "100% 23794/23794 [00:00<00:00, 24989.06it/s]\n",
            "train and test sequences are folded\n",
            "train and test inputs and targets are created\n",
            "model params are {'hidden_size': 32, 'item_vocab_size': 27616, 'dropout': 0.2, 'regulate_dot_product': True, 'normalize_embedding': True, 'time_decay': True}\n",
            "model is created\n",
            "model is compiled\n",
            "start fitting\n",
            "Epoch 1/20\n",
            "213/213 [==============================] - 7s 28ms/step - loss: 0.7366 - val_loss: 0.6490\n",
            "Epoch 2/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.6197 - val_loss: 0.5986\n",
            "Epoch 3/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5877 - val_loss: 0.5796\n",
            "Epoch 4/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5738 - val_loss: 0.5703\n",
            "Epoch 5/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5663 - val_loss: 0.5652\n",
            "Epoch 6/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5619 - val_loss: 0.5621\n",
            "Epoch 7/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5589 - val_loss: 0.5601\n",
            "Epoch 8/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5569 - val_loss: 0.5588\n",
            "Epoch 9/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5555 - val_loss: 0.5578\n",
            "Epoch 10/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5544 - val_loss: 0.5571\n",
            "Epoch 11/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5536 - val_loss: 0.5567\n",
            "Epoch 12/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5529 - val_loss: 0.5563\n",
            "Epoch 13/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5524 - val_loss: 0.5560\n",
            "Epoch 14/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5519 - val_loss: 0.5557\n",
            "Epoch 15/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5515 - val_loss: 0.5557\n",
            "Epoch 16/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5512 - val_loss: 0.5554\n",
            "Epoch 17/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5509 - val_loss: 0.5554\n",
            "Epoch 18/20\n",
            "213/213 [==============================] - 6s 27ms/step - loss: 0.5507 - val_loss: 0.5553\n",
            "Epoch 19/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5504 - val_loss: 0.5552\n",
            "Epoch 20/20\n",
            "213/213 [==============================] - 6s 28ms/step - loss: 0.5503 - val_loss: 0.5551\n",
            "test auc = 0.798191061357066 in cv = 4\n",
            "\n",
            " cv 4 finished for nips2020\n",
            "\n",
            "average test auc for nips2020 is 0.7986760733632614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vAZm924WreE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}